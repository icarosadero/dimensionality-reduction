{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cebra\n",
    "import torch\n",
    "import torch.utils\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from utils import TensorDataset, SimpleTensorDataset, SupervisedNNSolver, LazySimpleTensorDataset\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/14 11:05:36 WARN Utils: Your hostname, aigo resolves to a loopback address: 127.0.1.1; using 131.220.127.56 instead (on interface eno1)\n",
      "24/12/14 11:05:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/14 11:05:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.startTime', '1734185136737'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', '131.220.127.56'), ('spark.executor.memory', '10g'), ('spark.driver.memory', '10g'), ('spark.app.submitTime', '1734185136544'), ('spark.app.id', 'local-1734185137937'), ('spark.executor.id', 'driver'), ('spark.app.name', 'LocalCluster'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.driver.port', '32955'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"LocalCluster\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+----+--------------------+--------------------+-------------------+------------------+------------+--------------------+--------------------+\n",
      "|               time|         position|original_velocity|pump|         neural_data|                  dt|                 dx|          velocity|       index| positional_encoding|           file_name|\n",
      "+-------------------+-----------------+-----------------+----+--------------------+--------------------+-------------------+------------------+------------+--------------------+--------------------+\n",
      "|                0.0|340.5632161968261|14.70307693342713| 0.0|[0.01222460158169...|                 0.0|  340.5632161968261|               0.0|128849018880|0.016677520715069446|ID18150/Day2/Data...|\n",
      "|  1064.542299940903|203.8894319396973|22.85712617975521| 0.0|[0.05241657793521...|0.031245738184907168| 0.6967086747446842|22.297718511935173|128849052864|0.005392748172032213|ID18150/Day2/Data...|\n",
      "|0.03124573818435289|341.0519375040834|14.75032561138972| 0.0|[-0.0688618198037...| 0.03124573818435289|0.48872130725732177| 15.64121495142213|128849018881| 0.01714372201631348|ID18150/Day2/Data...|\n",
      "|  1064.573545679087|204.6144813062959|22.80167053192041| 0.0|[0.04073449224233...|0.031245738183997673| 0.7250493665985971|23.204744350380782|128849052865|-0.00697306839416...|ID18150/Day2/Data...|\n",
      "|0.06249147636870577|341.5541421054114|14.81840835398831| 0.0|[-0.0193288326263...|0.031245738184352882| 0.5022046013280033|16.072739212143027|128849018882|0.013447201157236514|ID18150/Day2/Data...|\n",
      "+-------------------+-----------------+-----------------+----+--------------------+--------------------+-------------------+------------------+------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load parquet files from the folder given by the environment variable $DATA_PATH\n",
    "parquet_dir = os.environ.get(\"DATA_PATH\")\n",
    "df = spark.read.format(\"parquet\").load(parquet_dir)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(time=10.40352990646771, position=303.7098174791827, original_velocity=22.97824965122394, pump=0.0, neural_data=[0.0013656221926794387, 0.0, 0.0017444707045797259, 0.000524658738868311, 0.002700238663237542, 0.0013478406617650762, 0.002971726018586196, 0.0013486453681252897, 0.000673954957164824, 0.0019533552695065737, 0.003248923283535987, 0.0019437470473349094, 0.006218665235792287, 0.007709247700404376, 0.0004077472403878346, 0.0014559410919900984, 0.013866361463442445, 0.0009866170221357606, 0.0025787973136175424, 0.0029110574687365443, 6.888731149956584e-05, 0.003609489751397632, 0.003379365080036223, 0.003814671259988245, 0.002384068808169104, 0.0, 0.002079587458865717, 0.0015136712609091774, 0.006746725179255009, 0.011375446803867817, 0.0, 0.0037978731270413846, 0.020811268594115973, 0.0004625998990377411, 0.0018709935975493863, 0.002631615920108743, 0.002259979388327338, 0.01983196916989982, 0.014820374548435211, 0.003057815949432552, 0.053889754228293896, 0.05039232806302607, 0.0056618009839439765, 0.002362559040193446, 0.00041107025390374474, 0.0005564712337218225, 0.03862707898952067, 0.0, 0.0005929417966399342, 0.0009188288386212662, 0.0, 0.40569424629211426, 0.0013703071235795505, 0.0, 0.029259446542710066, 0.0004890881828032434, 0.46424200385808945, 0.0018755219643935561, 0.10941275022923946, 0.0013501101202564314, 0.0, 0.0798261184245348, 0.0, 0.002112019748892635, 0.03984348429366946, 0.0030562512256437913, 0.002548637246945873, 0.0010936223261523992, 0.002397064206888899, 0.0, 0.0, 0.005035533817135729, 0.005097098066471517, 0.0028900542529299855, 0.0021699421195080504, 0.0022972367878537625, 0.0010231458290945739, 0.0, 0.0019311004143673927, 0.0005243830964900553, 0.0, 0.0, 0.0018998273299075663, 0.003007199615240097, 0.0, 0.00018075789557769895, 0.00011750988778658211, 0.0, 0.0010305307914677542, 0.0012803815479855984, 0.0013985209698148537, 3.080896567553282e-05, 0.00012876162145403214, 0.0, 0.0023114764189813286, 0.0014899902744218707, 0.0, 0.0012026392687403131, 0.003950070531573147, 0.002838086409610696, 0.0018340268234169343, 0.0018576764268800616, 0.005015930044464767, 0.1872144751250744, 0.0016261540295090526, 0.0022724666487192735, 0.005054675741121173, 0.004535319632850587, 0.0017338238540105522, 0.0, 0.0, 0.0, 0.0018194698059232906, 0.002862364795873873, 0.0, 0.0008633115339762298, 0.005600228265393525, 0.005587314837612212, 0.009868783527053893, 0.0, 0.021880907006561756, 0.0009655795875005424, 0.008883250877261162, 0.002841084453393705, 0.003166478150887997, 0.0031823807657929137, 0.008307736366987228, 0.005978508805128513, 0.0427219751290977, 0.005783894011983648, 0.0, 0.0010566138189460617, 0.0, 0.0013032951101195067, 0.0, 0.0006378703255904838, 0.00296686525689438, 0.003711020304763224, 0.017243568785488605, 0.0005399295100687596, 0.0, 0.0014961444539949298, 0.0, 0.0, 0.9349277317523956, 0.0024939142167568207, 0.01653031414025463, 0.004548504308331758, 0.009134554158663377, 0.00034559890627861023, 0.0029678427963517606, 0.0023690962079854216, 0.0, 0.0029606707976199687, 0.002644204389071092, 0.018603397416882217, 0.004419656877871603, 0.0011377715272828937, 0.00314511738542933, 0.0026637785194907337, 0.002680517063708976, 0.000404252961743623, 0.00017102283891290426, 0.0009585984153090976, 5.419334047473967e-05, 0.0035208516055718064, 0.0019591220370784868, 0.0015976451977621764, 0.002041669999016449, 0.0028154872270533815, 0.0009261526574846357, 0.003511055627313908, 0.0008974664087872952, 0.0006625257956329733, 0.0015962444304022938, 0.002987007101182826, 0.0011037819131161086, 0.005294439964927733, 0.00349882326554507, 0.0006410696059901966, 0.0028382845739542972, 0.00299875222845003, 0.004535385232884437, 0.0021087133209221065, 0.0018956489948322996, 0.005248530971584842, 0.046658425126224756, 0.0013949118729215115, 0.003549037195625715, 0.019066247681621462, 0.001416357612470165, 0.009204542904626578, 0.0, 0.0, 0.020686233590822667, 0.0629233350045979, 0.02017225744202733, 0.0049527635565027595, 0.005602904420811683, 0.0009181835339404643, 0.0028833934338763356, 0.03458638163283467, 0.0006410785936168395, 0.00593106824089773, 0.00447958931181347, 0.0, 0.0, 0.001107957272324711, 0.019231437938287854, 0.008839275047648698, 0.21324875205755234, 0.22608836740255356, 0.04193889023736119, 0.0, 0.0010099508363055065, 0.00184254283158225, 0.0014871218299958855, 0.001775424947936699, 0.005305900354869664, 0.00682700052857399, 0.003698095002619084, 0.0023263060611498076, 0.0063572259387001395, 0.0021398641110863537, 0.012989589798962697, 0.012545310892164707, 0.0013684295372513589, 0.04384723957628012, 0.06633876357227564, 0.03981274599209428, 0.0029935919719719095, 0.0065904841758310795, 0.003698181390063837, 0.0, 0.003494577802484855, 0.0030357710929820314, 0.0005631618696497753, 0.002553995786001906, 0.0, 0.0017898910155054182, 0.0009423368028365076, 0.0033139087972813286, 0.0003950558020733297, 0.003419905246119015, 0.006462796882260591, 0.026540501741692424, 0.001073473016731441, 0.005056140711531043, 0.0011111789326605503, 0.0031319730333052576, 0.002435376582070603, 0.0021596022197627462, 0.0031305603697546758, 0.0024661265251779696, 0.003661560287582688, 0.0011702276824507862, 0.0030940852884668857, 0.0, 0.001998316598474048, 0.0025061346677830443, 0.0067133334232494235, 0.18868440948426723, 0.0, 5.2056508138775826e-05, 0.0009587981185177341, 0.0010955118341371417, 0.0032856875332072377, 0.00928084400948137, 0.008097865735180676, 0.0, 0.002957289689220488, 0.04119784431532025, 0.003292696943390183, 0.0012285865668673068, 0.0, 0.0018855087691918015, 0.003742977205547504, 0.0, 0.0014491223846562207, 0.0006269582954701036, 0.0033358031178067904, 0.0030005328517290764, 0.002773944317596033, 0.0006672970994259231, 0.0027154588260600576, 0.004510853461397346, 0.0729089817032218, 0.007531877723522484, 0.0020824246312258765, 0.0032591454801149666, 0.0016333245439454913, 0.0006539918977068737, 0.0021057807171018794, 0.05851689027622342, 0.0, 0.0005561672733165324, 0.0009006850887089968, 0.004018668143544346, 0.004862076777499169, 0.002163719414966181, 0.0, 0.0018709324758674484, 0.004341478028436541, 0.002510029822587967, 0.004013314261101186, 0.0035516226180334343, 0.0053239145781844854, 0.0016563556782784872, 0.0012779735261574388, 0.0023145846971601713, 0.0015130066021811217, 0.0, 0.0031784333696123213, 0.0011352502421004829, 0.1227114163339138, 0.004668436173233204, 0.07608457282185555, 0.0062693812942598015, 0.0, 0.0018669151177164167, 0.01020393570070155, 0.0460021086037159, 0.0005196414422243834, 0.003584670295822434, 0.0, 0.0, 0.0, 0.0005472742268466391, 0.00027944524481426924, 0.0016891054983716458, 0.0, 0.0002017669758060947, 0.0, 0.010323083552066237, 0.08824388682842255, 0.0, 0.0, 0.0, 0.0007637821981916204, 0.011443076422438025, 0.0, 0.0, 0.010683760418032762, 0.0, 0.01367553067393601, 0.019919790094718337, 0.10235687531530857, 7.909324631327763e-05, 0.0, 0.003235364791180473, 0.0036583266337402165, 0.005816770324599929, 0.0005832128736074083, 0.0008363478318642592, 0.0, 0.004504303040448576, 0.0, 0.002681843296159059, 0.0029272758183651604, 0.002151960907212924, 0.0, 0.0, 0.002078752950183116, 0.0032273076940327883, 0.0014346949683385901, 0.0012562088813865557, 0.004252620754414238, 0.16169567219913006, 0.0, 0.01582862134091556, 0.0010665007284842432, 0.001192592884763144, 0.0, 0.0, 0.002257525222375989, 0.000576221224036999, 0.009639181662350893, 0.0009901949670165777, 0.01254127745050937, 0.0039437897721654736, 0.0004997991491109133, 0.0028921104822074994, 0.002665758950797681, 0.001740001971484162, 0.004427639491041191, 0.0017297410959145054, 0.0020392664591781795, 0.00020465478155529127, 0.0005534373340196908, 0.0005258434030110948, 0.0023447313287761062, 0.0056160274252761155, 0.0020260654637240805, 0.0, 0.0019528158591128886, 0.0, 0.0, 0.0, 0.011947716819122434, 0.004968870431184769, 0.0032004676540964283, 0.024903195444494486, 0.0010850571561604738, 0.0, 0.0030318148383230437, 0.07289872877299786, 0.0029463537503033876, 0.18310584500432014, 0.4067021980881691, 0.002795084576064255, 0.0038211100036278367, 0.009457638021558523, 0.5030015856027603, 0.17978171072900295, 0.18178193643689156, 0.0004534282488748431, 0.003742732893442735, 0.0, 0.0, 0.0, 8.026734576560557e-05, 0.0029014088650001213, 0.0013808943767799065, 0.002522844893974252, 0.0011659480660455301, 0.005611644097371027, 0.0032876812911126763, 0.0011498325839056633, 0.018993098638020456, 0.00394896196667105, 0.003002131765242666, 0.0020832099835388362, 0.0019077429024036974, 0.0020705365313915536, 0.006833704654127359, 0.0013578111247625202, 0.0016097374536911957, 0.009100894094444811, 0.0019504063675412908, 0.00200885022059083, 0.006149994791485369, 0.0021790824830532074, 0.008226089994423091, 0.0290652149124071, 0.003027755818038713, 0.09897147677838802, 0.08178952289745212, 0.003400638001039624, 0.002769658808119857, 0.003021583843292319, 0.0031068243843037635, 0.0017557301034685224, 0.002781476970994845, 0.002141367716831155, 0.01696039503440261, 0.005954781896434724, 0.008669398113852367, 0.009942901713657193, 0.029163908446207643, 0.001396135235154361, 0.0035273708344902843, 0.0012024992174701765, 0.012924947644933127, 0.014418333652429283, 0.2062310166656971, 0.01093301406945102, 0.13676663022488356, 0.012128030648455024, 0.004349618422565982, 0.0, 0.0030043892220419366, 0.0031955339945852757, 0.002758356189588085, 0.0, 0.002413708785752533, 0.0, 0.0013252236531116068, 0.013662909623235464, 0.0, 0.0014221542660379782, 0.025551058119162917, 0.0027356856007827446, 0.0005227201036177576, 0.009995647706091404, 0.008435014678980224, 0.008133784751407802, 0.0, 0.0, 0.0011671386891975999, 0.004123825594433583, 0.0, 0.0677305031567812, 0.02258238405920565, 0.00272838854289148, 0.0020013116882182658, 0.012910260818898678, 0.007670458639040589, 0.0, 0.0020050682651344687, 0.013144294469384477, 0.00027891769423149526, 0.002076784599921666, 0.0, 0.004554399085463956, 0.00313850722159259, 0.11430816911160946, 0.0023110085166990757, 0.003855495961033739, 0.0032483392496942542, 0.01937460266344715, 0.018488372792489827, 0.0017429180443286896, 0.004235892731230706, 0.002512487058993429, 0.0018523314065532759, 0.08164832089096308, 0.0, 0.004441586963366717, 0.012436515375156887, 0.013817899627611041, 0.00026992337370757014, 0.0008490407199133188, 0.003977228072471917, 0.0017039083322742954, 0.0025797240814426914, 0.00025156668561976403, 0.0008176051487680525, 0.007382287585642189, 0.017362649785354733, 6.220566865522414e-05, 0.0, 0.0020320676267147064, 0.017750545346643776, 0.003032640350284055, 0.0029333062302612234, 0.002237347565824166, 0.0007131584716262296, 0.00027141504688188434, 0.0037374728708527982, 0.0026219495266559534, 0.010332200792618096, 0.0022153892350615934, 0.0029859804490115494, 0.008558807079680264, 0.0008878223889041692, 0.04489761218428612, 0.0038595282239839435, 0.0011578657258723979, 0.0, 0.09253792185336351, 0.0061638364277314395, 0.003948026118450798, 0.004443775920663029, 0.0012307816214160994, 0.005414492694399087, 0.0067009099293500185, 0.0005277701857266948, 0.002757070236839354, 0.003992538935563061, 0.002084859449041687, 0.002520173031371087, 0.023271682672202587, 0.00012133797281421721, 0.001371305414068047, 0.0, 0.03751270845532417, 0.0, 0.0, 0.0, 0.0002801009395625442, 0.417278029024601, 0.0005840712110511959, 0.3977404609322548, 0.012517345952801406, 0.002873311226721853, 0.0, 0.005507316393050132, 0.0036498479603324085, 0.0024037353578023612, 0.0024504564644303173, 0.0048127710542757995, 6.250967271625996e-05, 0.0, 0.0019612990436144173, 0.003349637237306524, 0.0006048522918717936, 0.003333189361001132, 0.003329918548843125, 0.15036310255527496, 0.005184229492442682, 0.005837092062620286, 0.0, 0.0013555177429225296, 0.0029098459344822913, 0.014375215047039092, 0.11858930811285973, 0.1209227629005909, 0.004866459348704666, 0.003166133916238323, 0.0029799603289575316, 0.0014707096270285547, 0.00036128096689935774, 0.015239049680531025, 0.002107629959937185, 0.439628541469574, 0.0065533994929865, 0.005423175491159782, 0.0150000536814332, 0.0010291897342540324, 0.13339176401495934, 0.008973764313850552, 0.02342468802817166, 0.032271404401399195, 0.003149431291944893, 0.03158229525433853, 0.01548104512039572, 0.0018085845513269305, 0.0019620249076979235, 0.0036572568205883726, 0.004878311650827527, 0.014940608059987426, 0.0026620753633324057, 0.0016243033605860546, 0.3353266306221485, 0.0011148029007017612, 0.1422608494758606, 0.0013141029339749366, 0.004132495101657696, 0.0, 0.0, 0.0009668313432484865, 0.0034046821419906337, 0.0045829452719772235, 0.006712987407809123, 0.001230786059750244, 0.0, 0.00830503844190389, 0.16286693699657917, 0.003948950092308223, 0.0009858645789790899, 0.000627198038273491, 0.0036892093485221267, 0.013106131053064018, 0.0035328291705809534, 0.00847754837013781, 0.002541524991102051, 0.002088419381834683, 0.0, 0.0017671905588940717, 0.0013051309797447175, 0.0014447308858507313, 0.0, 0.0, 0.004953970812493935, 0.00451193971093744, 0.0, 1.09039647213649e-05, 0.0033056400206987746, 0.004829772908124141, 0.003615206675021909, 0.0024330698979611043, 0.002450904852594249, 0.007965060696733417, 0.0003409379642107524, 0.003455161349847913, 0.012340976274572313, 0.0014617257111240178, 0.0, 0.00047637148236390203, 0.0017284178175032139, 0.0013243490016066062, 0.0009326927538495511, 9.720848174765706e-05, 0.0036736899201059714, 0.004060643292177701, 0.0024740749067859724, 0.0020130135235376656, 0.00033549004001542926, 0.002166994738217909, 0.09275884181261063, 0.001626255427254364, 0.0, 0.0, 0.0006159044351079501, 0.03454468329437077, 0.07670161314308643, 0.002294199907922234, 0.01396648131776601, 0.009478694759309292, 0.001506590149801923, 0.0020184271270409226, 0.08609047718346119, 0.007258281868416816, 0.0022900872281752527, 0.0, 0.0009248487767763436, 0.01172602764563635, 0.0029755301657132804, 0.037945618852972984, 0.007721361413132399, 0.0673384009860456, 0.004007088457001373, 0.1263484563678503, 0.0021394231007434428, 0.00549723143922165, 0.046073528937995434, 0.027186134713701904, 0.001050301711075008, 0.0008213453693315387, 0.001709336880594492, 0.0013792894897051156, 1.1208016076125205e-05, 0.003648955447715707, 0.014820966869592667, 0.0019174539302184712, 0.0006475188674812671, 0.0, 0.0021334730699891225, 0.0030471779755316675, 0.0009399064583703876, 0.01360399772238452, 0.0007792645424160582, 0.060128697426989675, 0.011055925628170371, 0.0, 0.000859709907672368, 0.001456122869058163, 0.00036457559872360434, 0.0018383198221272323, 0.0, 0.0002962224607472308, 0.01599246724799741, 0.00380248659348581, 0.004481148011109326, 0.0033551357919350266, 0.0, 0.0026119984686374664, 0.0006871603618492372, 0.007588454580400139, 0.0, 0.1980251483619213, 0.0006132803682703525, 0.001582611454068683, 0.0, 0.002041866653598845, 0.0025549375495756976, 0.004030116724607069, 0.0025513820874039084, 0.002387940156040713, 0.0011863432300742716, 0.06410844065248966, 0.07751866430044174, 0.001615073637367459, 0.002080822734569665, 0.0026784591609612107, 0.002167656348319724, 0.0019088782646576874, 0.0014863767646602355, 0.001999945205170661, 0.0018408858159091324, 0.0017813854792620987, 0.0016211428446695209, 0.0, 0.0005163304631423671, 0.0019010119685844984, 0.0, 0.004725884733488783, 0.0006606556999031454, 0.0017980777338379994, 0.0012311942700762302, 0.00039167035720311105, 0.006795117980800569, 0.0011001877865055576, 0.0012696050107479095, 0.002531839178118389, 0.00023740016331430525, 0.004963392741046846, 0.012413797434419394, 0.002055230113910511, 0.003024467107024975, 0.0013163827861717436, 0.0011967085883952677, 0.00010875402949750423, 0.003953681152779609, 0.0016509853885509074, 0.00229251166456379, 0.029107192996889353, 0.034442160511389375, 0.00023367636458715424, 0.002122376172337681, 0.0009433351806364954, 0.0, 0.0005615406989818439, 0.008616649429313838, 0.0011135183740407228, 0.005193141754716635, 0.0020230941590853035, 0.001987842540984275, 0.0037032894615549594, 0.0018523387552704662, 0.0018486269764252938, 0.001165587455034256, 0.05829039728268981, 0.0, 0.0021282695524860173, 0.0014599673449993134, 0.0015418343027704395, 0.003319814168207813, 0.0019155779054926825, 0.0, 0.0014559530682163313, 0.0012458839105420338, 0.01825937640387565, 0.0004604645546351094, 0.0006969743208173895, 0.0, 0.00160864968347596, 0.0001990832679439336, 0.0019818447763100266, 0.0017361336158501217, 0.0, 0.0004121340753044933, 0.0005056940717622638, 0.059401702135801315, 0.005853456630575238, 0.0012925334012834355, 0.0005453311750898138, 0.0015711941305198707, 0.0013546548871090636, 0.0014226980856619775, 0.003477171889244346, 0.0005524659063667059, 0.0019943409133702517, 0.0015134104469325393, 0.005417519074399024, 0.0020965017392882146, 0.002759187715128064, 0.0014654867409262806, 0.004117322736419737, 0.0068308450281620026, 0.0018675439932849258, 0.0014470763853751123, 0.004322028951719403, 0.0023566120071336627, 0.0008514457149431109, 0.00115716083382722, 0.0019995576731162146, 0.009224680019542575, 0.006003432674333453, 0.0019487523386487737, 0.003026060039701406, 0.00044752292888006195, 0.035862268414348364, 0.0010095648540300317, 0.003433682839386165, 0.003532726339471992, 0.0015825547015992925, 0.0, 0.004697194031905383, 0.0, 0.0010683217115001753, 0.002538713102694601, 0.11642447672784328, 0.0025327719922643155, 0.0, 0.0, 0.15062620677053928, 0.003289877640781924, 0.001392009075061651, 0.07150719128549099, 0.0005711876146961004, 0.024515062337741256, 0.01562442013528198, 0.35970350354909897, 0.01032451936043799, 0.0014234342088457197, 0.0005181499000173062, 0.001672320780926384, 0.0, 0.012718679965473711, 0.005199787789024413, 0.0019133695423079189, 0.0009868048582575284, 0.0031490109831793234, 0.0037219890509732068, 0.001304217497818172, 0.0, 0.02651513717137277, 0.0006241106893867254, 0.002788668127323035, 0.0006538920424645767, 0.005200646119192243, 0.0005636074492940679, 0.0038671665242873132, 0.011712980456650257, 0.0024721943627810106, 0.0016561591037316248, 0.028091748477891088, 0.0013298697595018893, 0.0034969635162269697, 0.0008375816396437585, 0.0013466917735058814, 0.00021210397244431078], dt=0.031241831550961052, dx=0.7851506941264574, velocity=25.131391315702324, index=300, positional_encoding=0.014916037285670205, file_name='ID17905/Day5/DataFrame_Imaging_spiking_17905_day5'),\n",
       " Row(time=13.52771306156311, position=69.53608138248562, original_velocity=44.26677130314268, pump=0.0, neural_data=[0.0006208472914295271, 0.0006064810149837285, 9.194247832056135e-05, 0.0017177605186589062, 0.004633741918951273, 0.0016687773313606158, 0.0010019846231443807, 0.0, 0.0004698999837273732, 0.01105173968244344, 0.0026143931099795736, 0.003396698120923247, 0.0010717354831513148, 0.0, 0.001321739749982953, 0.002299182931892574, 0.002709642855961647, 0.002056161407381296, 0.0002613642136566341, 0.0022573061869479716, 0.003911860811058432, 0.0025052585697267205, 0.0010126583874807693, 0.0008842863462632522, 0.0469190776348114, 0.003417732601519674, 0.0023045026464387774, 0.004884339101408841, 0.002621287858346477, 0.0040596921935502905, 0.003360285598319024, 0.0, 0.0001343943295069039, 0.003106086762272753, 0.3665900379419327, 0.01536567194852978, 0.001877117989351973, 0.0, 0.0010587922733975574, 0.0011146136166644283, 0.001736398902721703, 0.34550004824995995, 0.0922119552269578, 1.5405661542899907e-05, 0.0, 0.002543098547903355, 0.012017259286949411, 0.007948561978992075, 0.0, 0.001188053960504476, 0.0, 0.005008278094464913, 0.00672654144000262, 0.0, 0.003932292078388855, 0.0, 0.0046951549156801775, 0.0, 0.0, 0.0003363351097505074, 0.0032453917956445366, 0.03848044853657484, 0.0, 0.0, 0.003330939303850755, 0.0011066131592087913, 0.19299807213246822, 0.0, 0.004180022355285473, 0.0025543637748342007, 0.0, 0.00035638734698295593, 0.002371322458202485, 0.0, 0.003183686116244644, 0.0012787244922947139, 0.0014309562866401393, 0.0, 0.002078168938169256, 0.0015379630494862795, 0.001293561639613472, 0.0024197422462748364, 0.0038024626846890897, 0.012182181351818144, 0.0054102725989650935, 0.002131816087057814, 0.005031951492128428, 0.0002400093071628362, 0.0020602278382284567, 0.005415722087491304, 0.003690812853164971, 0.0025250502658309415, 0.0010483199002919719, 0.006306675873929635, 0.0004241429560352117, 0.0009549312671879306, 0.0005033009147155099, 0.004120857222005725, 0.004213153646560386, 0.0021791004692204297, 0.0, 0.00027476774994283915, 0.00034112123830709606, 0.0015288912109099329, 0.0, 0.0005225056302151643, 0.007424630457535386, 0.0020235427473380696, 0.11533264629542828, 0.0204812902957201, 0.0, 0.0003746443326235749, 0.004416169918840751, 0.0, 0.006892659526783973, 0.0002197825233452022, 0.0013040088524576277, 0.003844168037176132, 0.0, 0.0, 0.0, 0.0004956118937116116, 0.0025216185313183814, 0.0013410963401838671, 0.00014447204011958092, 0.0, 0.0018416024249745533, 0.003735091653652489, 0.0, 0.00021567877411143854, 0.0, 0.00012704357504844666, 0.002877063423511572, 0.0015230886929202825, 0.047965023666620255, 0.0, 0.0, 0.0018557212970335968, 0.0011283867534075398, 0.0, 0.0, 0.0, 0.0009165205410681665, 0.0, 0.06347631523385644, 0.0008620180888101459, 0.0, 0.0, 0.05590308457612991, 0.0011203892645426095, 0.006325437861960381, 0.0, 0.002065985681838356, 0.0014140018392936327, 0.04560709185898304, 0.003911674895789474, 0.002097611897625029, 0.0014258512383094057, 0.001778195088263601, 0.003173104691086337, 0.0, 0.003105249284999445, 0.0, 0.0006420813879230991, 0.007673432247429446, 0.0014150314018479548, 0.0020800852216780186, 0.002791834757772449, 0.001693321049970109, 0.0018744089393294416, 0.003224123327527195, 0.004604617610311834, 0.0013305299180501606, 0.0024745248665567487, 0.003999912092695013, 0.002009040952543728, 0.001956159685505554, 0.001457049758755602, 0.0010305874529876746, 0.001565595761348959, 0.0004944586398778483, 0.003775133949602605, 0.001081488277122844, 0.0019214384756196523, 0.0038695894763804972, 0.0, 0.0, 0.0, 0.009999437374062836, 0.005262271384708583, 0.0, 0.0006697081116726622, 0.0013469660189002752, 0.003314639034215361, 0.001976830681087449, 0.0, 0.0014713488635607064, 0.011699336464516819, 0.12396574392914772, 0.007141752808820456, 0.002982144862471614, 0.25712781958281994, 0.014802342164330184, 0.00027985800988972187, 0.003252893016906455, 0.0, 0.01965239131823182, 0.0, 0.0, 0.0038089729787316173, 0.0, 0.0, 0.0, 0.007536546210758388, 0.0, 0.020248682238161564, 0.002253725513583049, 0.0, 0.0, 0.0, 0.00047628069296479225, 0.0009686433186288923, 0.01701625983696431, 0.0, 0.22781658545136452, 0.0011735984007827938, 0.0024272181908600032, 0.0, 0.0, 0.0046751657500863075, 0.00151061645738082, 0.004272484129614895, 0.004556909108941909, 0.0037755556404590607, 0.0009224646346410736, 0.0054950647172518075, 0.003211738934624009, 0.0013095163449179381, 0.0018538765725679696, 0.002934644478955306, 0.0, 0.0024055444155237637, 0.0024834998876031023, 0.003998856991529465, 0.0028858788136858493, 0.039757146674674004, 0.0062863207422196865, 0.01781830529216677, 0.4706200286746025, 0.006045821995940059, 0.045324889943003654, 0.008436192641966045, 0.0008217864669859409, 0.00360025986446999, 0.0, 0.003142344179650536, 0.0, 0.003654021813417785, 0.0007813878764864057, 0.0, 0.0007516853656852618, 0.0001415245933458209, 0.0, 0.0002192491083405912, 0.0005538486584555358, 0.005001244833692908, 0.0, 0.003322457559988834, 0.00030810903990641236, 0.008126787783112377, 0.005281824225676246, 0.0, 0.001688919251137122, 0.007966044766362756, 0.0, 0.0, 0.11046536266803741, 0.0, 0.002720940927247284, 0.0038659208512399346, 0.0018688596319407225, 0.0002045609726337716, 0.003503646730678156, 0.0, 0.0, 0.01428917155135423, 0.0, 0.0011913009220734239, 0.002358285419177264, 0.0028755597886629403, 0.005303851503413171, 0.006322422006633133, 0.0, 0.002404647384537384, 0.11960785463452339, 0.00016481515194755048, 0.0, 0.0017946147127076983, 0.0018162422857130878, 0.007387843215838075, 0.002784379670629278, 0.0010976098565151915, 0.007195244077593088, 0.0006886609189677984, 0.006456022761994973, 0.0031265057623386383, 0.008679136051796377, 0.0004938862985000014, 0.004378623561933637, 0.0003871076478390023, 0.0, 0.0019662864615384024, 0.0016718261867936235, 0.0010008298413595185, 0.01051343655853998, 0.026101234834641218, 0.001872193104645703, 0.002438852869090624, 0.026492664590477943, 0.0012048475764459, 0.0016345569310942665, 0.0, 0.04044876043917611, 0.002311601616383996, 0.0013349689761525951, 0.0, 0.0008336627870448865, 0.0029628255433635786, 0.007733410864602774, 0.0011436426138971, 0.0005528637120733038, 0.0, 0.147436099126935, 0.0007097624475136399, 0.0, 0.009472639019804774, 0.0016453198622912169, 0.0, 0.0012856252724304795, 0.0020440929220058024, 0.002053842748864554, 0.04739240510389209, 0.8667724132537842, 0.17409253679215908, 0.019048073911108077, 0.0038254820465226658, 0.01470004563452676, 0.0012443237792467698, 0.0012296669883653522, 0.001146800146671012, 0.0022718407562933862, 0.0032922573809628375, 0.005313968093105359, 0.0, 0.0031329222765634768, 0.005186053458601236, 0.01612509577535093, 0.0015519984881393611, 0.0010550387669354677, 0.0017228372234967537, 0.0012727163739327807, 0.006677914505416993, 0.0017190419966937043, 0.0045042910787742585, 0.0007637240560143255, 0.001567324285133509, 0.0011842005824291846, 0.001313718457822688, 0.010048719763290137, 0.0, 0.0011128195037599653, 0.21918530017137527, 0.003788601723499596, 0.0, 0.0018656917600310408, 0.025703371735289693, 0.0018171721167163923, 0.0017871924210339785, 0.0, 0.002322907002962893, 0.0, 0.0002674557763384655, 0.0019213089108234271, 0.0019448094280960504, 0.0014406644186237827, 0.002681765785382595, 0.0009809024777496234, 0.0012054287653882056, 0.003501775110635208, 0.002340830134926364, 0.0008458413649350405, 0.006304594222456217, 0.00163839096785523, 0.00047000580525491387, 0.002445396083203377, 0.003853743488434702, 0.006827657693065703, 0.0002890848845709115, 0.0, 0.0, 0.0004831963451579213, 0.0004402497143018991, 0.0025266685624956153, 0.08024095464497805, 0.0006350733747240156, 0.00011952151544392109, 0.0023087524314178154, 0.004011087818071246, 0.0034464203636161983, 0.008678030746523291, 0.09842265583574772, 0.0, 0.00814549726783298, 0.07777054514735937, 0.005009173648431897, 0.00193663765094243, 0.0, 0.001443775705411099, 0.3144919089972973, 0.005328174775058869, 0.04355073696933687, 0.0015181563649093732, 0.008857757551595569, 0.0019026883310289122, 0.0017027698922902346, 0.003589903593820054, 0.00025200474192388356, 0.000642734143184498, 0.0013696796959266067, 0.004092199407750741, 0.0012211196299176663, 0.00220054027158767, 0.0018023349293798674, 0.003916972258593887, 0.001944967347299098, 0.0008657359867356718, 0.0021839984401594847, 0.010660497820936143, 0.0028768801275873557, 0.005869227043149294, 0.0034481369657441974, 0.0014609866484533995, 0.0018871613428927958, 0.0008413798932451755, 0.0023148664040490985, 0.03294338099658489, 0.032650608802214265, 0.0014779233315493912, 0.0018294476030860096, 0.00217352435720386, 0.004811962833628058, 0.0013614125405183586, 0.00331463745169458, 0.0, 0.14426138252019882, 0.003595816422603093, 0.00392062593891751, 0.005715101375244558, 0.0019430480897426605, 0.003273401462138281, 0.023329450748860836, 0.003520271959132515, 0.042558388551697135, 0.02458492061123252, 0.0, 1.0199051052331924, 0.06762081477791071, 0.027047670213505626, 0.002581538981758058, 0.0791326304897666, 0.0043453195357869845, 0.008591725840233266, 0.0, 0.0032656900148140267, 0.0, 0.010339701897464693, 0.0, 0.0002856411301763728, 0.002948650042526424, 0.0037397213018266484, 0.0011516131926327944, 0.0, 0.002564551636169199, 0.0021264233073452488, 0.3319745622575283, 0.026093784254044294, 0.0028031029214616865, 0.001590680389199406, 0.031242439057677984, 0.0009273760078940541, 0.014648545417003334, 0.005288934022246394, 0.005891524982871488, 0.0006077402067603543, 0.0021483101591002196, 0.0016097344341687858, 0.004007294133771211, 0.010143154533579946, 0.0, 0.0, 0.0, 0.0014651842648163438, 0.002008865005336702, 0.00046257604844868183, 0.0049842574808280915, 0.004097267308679875, 0.05910058505833149, 0.001351522655795634, 0.0012544640339910984, 0.0, 0.00466371449874714, 0.001216816235682927, 0.0, 0.014859886490739882, 0.0016924568044487387, 0.01199162914417684, 0.00587646453641355, 0.00018200061822426505, 0.0, 0.2282738834619522, 0.0, 0.0004874758960795589, 0.0010845699289347976, 0.0005889289604965597, 0.011379730072803795, 0.5160836279392242, 0.0, 0.0021639877013512887, 0.0, 0.0, 0.009926783794071525, 0.004877773695625365, 0.0018187352106906474, 0.0018937729473691434, 0.11339648067951202, 0.0038072163006290793, 0.0025797315465752035, 0.0019098885750281624, 0.0004717357751360396, 0.00046450874651782215, 0.03230804647319019, 0.004594563715727418, 0.0020723365887533873, 0.0010998956859111786, 0.0001460753846913576, 0.0, 0.0016206376312766224, 0.0006413363298634067, 0.05422577587887645, 0.007999508758075535, 0.0, 0.15970025584101677, 0.1426528748124838, 0.01904700812883675, 0.0, 0.040005396120250225, 0.0032646723793732235, 0.0004649766269722022, 0.0021752410611952655, 0.007589826767798513, 0.0, 0.0, 0.0010337822895962745, 0.0014594503154512495, 0.0006202170043252409, 0.001628484678803943, 0.0007951864390634, 0.0027511529624462128, 0.0, 0.003938252571970224, 0.00030780062661506236, 0.0005338393821148202, 0.0039261720012291335, 0.0006749608437530696, 0.008206908707506955, 0.004139804223086685, 0.00533537392038852, 0.10994145646691322, 0.0025054864236153662, 0.0045103916636435315, 0.0013579674996435642, 0.0, 0.0034775771709973924, 0.0005950533522991464, 0.0, 0.00015486702613998204, 0.004643986379960552, 0.004991347341274377, 0.0, 0.0020114176186325494, 0.008733829716220498, 0.056635665241628885, 0.20649304240942, 0.0014379047570400871, 0.02047708013560623, 0.01646576216444373, 0.041254147654399276, 0.0027129031714139273, 0.0, 0.37324225902557373, 0.0011847295390907675, 0.003114438892225735, 0.023149888729676604, 0.0022345535398926586, 0.03485792875289917, 0.0, 0.0, 0.01243264036020264, 0.001309164632402826, 0.0063003788236528635, 0.0006272786522458773, 0.006391682487446815, 0.0, 0.0, 0.00510575546650216, 0.006298305204836652, 0.0, 0.04321266803890467, 0.0014974945224821568, 0.008320190419908613, 0.0, 0.001895614739623852, 0.004908863484160975, 0.002085138257825747, 0.006694462150335312, 0.0, 0.001504398140241392, 0.0, 0.0, 0.02169142314232886, 0.0005415373598225415, 0.025905561866238713, 0.0012173139111837372, 0.0003525518550304696, 0.0010085754620376974, 0.028063711477443576, 0.002015424528508447, 0.0, 0.0008120440688799135, 0.014136086567305028, 0.002108617904013954, 0.023674314667005092, 0.0008587532211095095, 0.0017605432076379657, 0.003111536098003853, 0.002599648491013795, 0.0027067118790000677, 0.001808356522815302, 0.013441324234008789, 0.003050967410672456, 0.0025894276914186776, 0.0019119836506433785, 0.0, 0.0023313455749303102, 0.005090176360681653, 0.0, 0.0006631993528571911, 0.0033016256929840893, 0.002790778744383715, 0.001156360754976049, 0.00417380704311654, 0.005378232985094655, 0.004956031567417085, 0.002676855794561561, 0.0, 0.0010703154621296562, 0.0013775907682429533, 0.0012426607718225569, 0.0, 0.00606481364229694, 0.0016571663436479867, 0.004841809801291674, 0.0042100675564142875, 0.0011434384796302766, 0.0033745758701115847, 0.0011080347612733021, 0.0007965231197886169, 0.0, 0.011370986001566052, 0.011249459814280272, 0.0083883844781667, 0.18806198798120022, 0.01813930249772966, 0.004848390133702196, 0.08412929018959403, 0.0, 0.002330020899535157, 0.010636728256940842, 0.0024260603240691125, 0.0008738405886106193, 0.0008964189328253269, 0.009348637133371085, 0.2599775604903698, 0.00039956203545443714, 0.003015225491253659, 0.0028014294803142548, 0.0036735646426677704, 0.0, 0.06056933291256428, 0.0, 0.0, 0.0, 0.0011575826065381989, 0.001543042846606113, 0.015746528515592217, 0.06082666851580143, 0.0004934512980980799, 0.000570916454307735, 0.0023196213005576283, 0.0027980722370557487, 0.0022999950233497657, 0.0010349445874453522, 0.00655210402328521, 0.004470815722015686, 0.00672702158044558, 0.0, 0.005461364133225288, 0.0, 0.29723507910966873, 0.003358755318913609, 0.008890618604709744, 0.005428578791907057, 0.0004212704370729625, 0.0007350587111432105, 0.04796059150248766, 0.0, 0.0008077212260104716, 0.04653350496664643, 0.12360167503356934, 0.0008090757819445571, 0.0002695121365832165, 0.002899695187807083, 0.0005390055812313221, 0.001191400777315721, 0.0, 0.0, 0.004711895453510806, 0.29880213737487793, 0.24148199707269669, 0.01235368890047539, 0.0007863261416787282, 0.0, 0.0018407590687274933, 0.0017828606942202896, 0.002056229888694361, 0.0010288544581271708, 0.0013699569390155375, 0.0023939930251799524, 0.0033817000512499362, 0.0006591237906832248, 0.031209294451400638, 0.0, 0.002769056780380197, 0.044795009307563305, 0.0022021108234184794, 0.0014893286061123945, 8.815748151391745e-05, 0.00177466357126832, 0.00221206919013639, 0.00033142044048872776, 0.0013943620506324805, 0.0, 0.0032207005424425006, 0.0024793259690341074, 0.12981964834034443, 0.0013346336636459455, 0.0006836168831796385, 0.0025548227422405034, 0.0032752119877841324, 0.0, 0.0, 0.0031033165578264743, 0.006419782992452383, 0.0018667333060875535, 0.0016091026554931886, 0.001095561048714444, 0.0034534774931671564, 0.00046830112114548683, 0.0006107471417635679, 0.07109171617776155, 0.000869061826961115, 0.0, 0.0006073959666537121, 0.00022547464322997257, 0.003348144207848236, 0.0, 0.0, 0.0015107281651580706, 0.0, 0.000856735568959266, 0.0014564208686351776, 0.0, 0.009602430742233992, 0.005887658859137446, 0.0019076460012001917, 0.00011433889449108392, 0.0024933406384661794, 0.0372717184945941, 0.0, 0.0017660789380897768, 0.0023486592472181655, 0.0, 0.03067074343562126, 0.0, 0.000824745831778273, 0.01727431546896696, 0.00028087582904845476, 0.0, 0.0017132335124188103, 0.16361944563686848, 0.0019390099187148735, 0.0033668377436697483, 0.01138583931606263, 0.001469948205340188, 0.0, 0.0021583669004030526, 0.00046235547051765025, 0.0035758920712396502, 0.0009212576260324568, 0.0047580563459632685, 0.07977010961622, 0.004211773783026729, 0.001702866723690022, 0.0, 0.004825037030968815, 0.0026568107277853414, 0.0033053555380320176, 0.002579855194198899, 0.0, 0.002445032456307672, 0.01288904232205823, 0.030908540647942573, 0.0021794729254906997, 0.0, 0.00363061367534101, 0.0, 0.04584858566522598, 0.0, 0.0007414453284582123, 0.0, 0.008756990917390794, 0.0027536653797142208, 0.0016240768891293555, 0.0010059245250886306, 0.0008412324677919969, 0.0017339163314318284, 0.0, 0.004387968179798918, 0.001938187429914251, 0.002463469689246267, 0.0, 0.0014835187575954478, 0.0040869302829378285, 0.002500466325727757, 0.008787444501649588, 0.0, 0.0025764756719581783, 0.20500008016824722, 0.001587056414791732, 0.002322793827715941, 0.004680302052292973, 0.002069928581477143, 0.0014775418676435947, 0.010816893656738102, 0.007298453991097631, 0.0018133276025764644, 0.8898024410009384, 0.003846458319458179, 0.0003290243912488222, 0.0, 0.0009955504210665822, 0.00638564967084676, 0.0017959527685889043, 0.0004475080349948257, 0.003201349842129275, 0.0011947013990720734, 0.0, 0.0, 0.0013636434159707278, 0.0032356612937292084, 0.0011178188724443316, 0.0, 0.0031721704290248454, 0.002723301287915092, 0.004227732977597043, 0.0019438720773905516, 0.002673067996511236, 0.041430281242355704, 0.0, 0.005213191407165141, 0.0028765969473170117, 2.3397034965455532e-05, 0.001546757761389017, 0.0027968488575425, 0.0032585360313532874, 0.0019924990920117125, 0.0023359455626632553, 0.002253448305054917, 0.003217885654521524, 0.0, 0.0026373155997134745, 0.002261701985844411, 0.004461506789084524], dt=0.031241831550950394, dx=1.1037271523135672, velocity=35.32850340459605, index=400, positional_encoding=0.007134074718487838, file_name='ID17905/Day5/DataFrame_Imaging_spiking_17905_day5')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.index.isin([300,400])).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 5000\n",
    "def get_x_ticks(L:int):\n",
    "    x_ticks = np.arange(0,L,delta)\n",
    "    x_tick_labels = [f\"{t[i]/100:.2f}\" for i in x_ticks]\n",
    "    return x_ticks, x_tick_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52796, 709), (13198, 709))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(0.2*len(X))\n",
    "X_train, X_test = X[:-split], X[-split:]\n",
    "y_train, y_test = df[\"Pump\"].values[:-split], df[\"Pump\"].values[-split:]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ticks, x_tick_labels = get_x_ticks(X.shape[0])\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "# ax = sns.heatmap(X.T, ax=ax, cmap=\"gray_r\")\n",
    "# ax.set_xticks(x_ticks)\n",
    "# ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "# v_bar = X_train.shape[0]\n",
    "# ax.axvline(v_bar, color=\"red\")\n",
    "\n",
    "# ax.set_title(\"Train and Test split\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEBRA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with device:\n",
    "    X_train_tensor = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "    X_test_tensor = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    y_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    y_test_tensor = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, model_name, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_units, latent_dimension = 8):\n",
    "        self.model_name = model_name\n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        self.X_test_tensor = X_test_tensor\n",
    "        self.y_test_tensor = y_test_tensor\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.num_units = num_units\n",
    "        self.train_dataset = TensorDataset(\n",
    "            neural = X_train_tensor,\n",
    "            discrete = y_train_tensor,\n",
    "        )\n",
    "        self.test_dataset = TensorDataset(\n",
    "            neural = X_test_tensor,\n",
    "            discrete = y_test_tensor\n",
    "        )\n",
    "        with device:\n",
    "            self.model = cebra.models.init(\n",
    "                name = model_name,\n",
    "                num_neurons = self.train_dataset.neural.shape[1],\n",
    "                num_units = num_units,\n",
    "                num_output = latent_dimension\n",
    "            )\n",
    "            self.train_dataset.configure_for(self.model)\n",
    "            self.test_dataset.configure_for(self.model)\n",
    "    def train_embedding(self, learning_rate, batch_size, steps = 1000, verbose = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = steps\n",
    "        with device:\n",
    "            criterion = cebra.models.criterions.LearnableCosineInfoNCE()\n",
    "            optimizer = torch.optim.Adam(\n",
    "                list(self.model.parameters()) + list(criterion.parameters()),\n",
    "                lr = learning_rate\n",
    "            )\n",
    "            self.embedding_solver = cebra.solver.SingleSessionSolver(\n",
    "                model = self.model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                tqdm_on = verbose\n",
    "            )\n",
    "            train_loader = cebra.data.single_session.DiscreteDataLoader(\n",
    "                dataset = self.train_dataset,\n",
    "                num_steps = steps,\n",
    "                batch_size = batch_size,\n",
    "                prior = \"empirical\"\n",
    "            )\n",
    "            self.embedding_solver.fit(loader=train_loader)\n",
    "    def train_decoder(self, verbose = True):\n",
    "        with device:\n",
    "            self.simple_train_dataloader = torch.utils.data.DataLoader(\n",
    "                SimpleTensorDataset(\n",
    "                    data = self.X_train_tensor.type(torch.FloatTensor),\n",
    "                    labels = self.y_train_tensor.type(torch.FloatTensor),\n",
    "                    offset = self.embedding_solver.model.get_offset(),\n",
    "                    device = device\n",
    "                ),\n",
    "                batch_size = self.batch_size,\n",
    "                shuffle = True\n",
    "            )\n",
    "            self.binaryClassifier = torch.nn.Sequential(\n",
    "                self.model,\n",
    "                torch.nn.Linear(self.latent_dimension,self.latent_dimension),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(self.latent_dimension,1),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(1,1) #Logit output\n",
    "            ).to(device)\n",
    "            self.decoder_solver = SupervisedNNSolver(\n",
    "                model = self.binaryClassifier,\n",
    "                criterion = torch.nn.BCEWithLogitsLoss(),\n",
    "                optimizer = torch.optim.Adam(self.binaryClassifier.parameters(), lr = self.learning_rate)\n",
    "            )\n",
    "            self.decoder_solver.fit(self.simple_train_dataloader, num_steps = self.steps)\n",
    "    def train(self, learning_rate, batch_size, steps, verbose = True):\n",
    "        self.train_embedding(learning_rate, batch_size, steps, verbose)\n",
    "        self.train_decoder(verbose = verbose)\n",
    "    def score(self, verbose = True):\n",
    "        with device:\n",
    "            test_loader = cebra.data.single_session.DiscreteDataLoader(\n",
    "                dataset = self.test_dataset,\n",
    "                num_steps = self.steps,\n",
    "                batch_size = self.batch_size,\n",
    "                prior = \"empirical\"\n",
    "            )\n",
    "            self.embedding_score = self.embedding_solver.validation(test_loader)\n",
    "            self.simple_test_dataloader = torch.utils.data.DataLoader(\n",
    "                SimpleTensorDataset(\n",
    "                    data = self.X_test_tensor.type(torch.FloatTensor),\n",
    "                    labels = self.y_test_tensor.type(torch.FloatTensor),\n",
    "                    offset = self.embedding_solver.model.get_offset(),\n",
    "                    device = device\n",
    "                    ),\n",
    "                batch_size = self.batch_size,\n",
    "                shuffle = True\n",
    "                )\n",
    "            self.decoder_score = self.decoder_solver.validation(self.simple_test_dataloader)['total']\n",
    "            try:\n",
    "                # Fit an exponential decay to the history and get the R2 value\n",
    "                def exp_decay(x, a, b, c):\n",
    "                    return a * np.exp(-b * x) + c\n",
    "\n",
    "                # Fit the exponential decay to the solver history\n",
    "                x_data = np.arange(len(self.embedding_solver.history))\n",
    "                y_data = self.embedding_solver.history\n",
    "                y0 = y_data[0]\n",
    "                yf = y_data[-1]\n",
    "                popt, _ = curve_fit(exp_decay, x_data, y_data, p0=(y0 - yf, 1e-6, yf))\n",
    "\n",
    "                # Calculate the R2 value\n",
    "                y_pred = exp_decay(x_data, *popt)\n",
    "                r2 = r2_score(y_data, y_pred)\n",
    "                if verbose:\n",
    "                    print(f\"Expontial decay R2: {r2}, embedding score: {self.embedding_score}, decoder score: {self.decoder_score}\")\n",
    "                return -r2 + self.embedding_score + 2*self.decoder_score\n",
    "            except:\n",
    "                return float(\"inf\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: -0.9850 neg:  3.4666 total:  2.4816 temperature:  0.9968: 100%|| 10/10 [00:01<00:00,  5.97it/s]\n",
      "100%|| 10/10 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expontial decay R2: 0.1465373380689179, embedding score: 2.484728240966797, decoder score: 0.8064513206481934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.951093544194266"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = ModelPipeline(\n",
    "    \"offset36-model\",\n",
    "    X_train_tensor=X_train_tensor,\n",
    "    y_train_tensor=y_train_tensor,\n",
    "    X_test_tensor=X_test_tensor,\n",
    "    y_test_tensor=y_test_tensor,\n",
    "    latent_dimension=8,\n",
    "    num_units=2\n",
    ")\n",
    "pipeline.train(learning_rate = 1e-3, batch_size = 12, steps = 10)\n",
    "pipeline.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('offset10-model', 'offset36-model', 'offset1-model', 'offset5-model')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_try = ('offset10-model', 'offset36-model', 'offset1-model')\n",
    "models_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(trial):\n",
    "    model_name = trial.suggest_categorical(\"model_name\", models_to_try)\n",
    "    num_units = trial.suggest_int(\"num_units\", 1, X_train.shape[1])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 50, 256, log=True)\n",
    "    model = ModelPipeline(\n",
    "        model_name,\n",
    "        X_train_tensor=X_train_tensor,\n",
    "        y_train_tensor=y_train_tensor,\n",
    "        X_test_tensor=X_test_tensor,\n",
    "        y_test_tensor=y_test_tensor,\n",
    "        num_units=num_units,\n",
    "        latent_dimension=8\n",
    "    )\n",
    "    model.train(learning_rate, batch_size, 500)\n",
    "    score = model.score()\n",
    "    \n",
    "    # Save the best model to disk\n",
    "    if not hasattr(experiment, \"best_score\") or score < experiment.best_score:\n",
    "        experiment.best_score = score\n",
    "        experiment.pipeline = model\n",
    "        torch.save(model.model.state_dict(), \"./data/models/best_model.pth\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-29 15:16:13,326] Using an existing study with name 'cebra_offsets' instead of creating a new one.\n",
      "pos: -0.9735 neg:  5.9531 total:  4.9797 temperature:  0.9370: 100%|| 500/500 [04:10<00:00,  2.00it/s]\n",
      "[W 2024-11-29 15:20:34,993] Trial 1 failed with parameters: {'model_name': 'offset5-model', 'num_units': 229, 'learning_rate': 0.00016581411988856763, 'batch_size': 163} because of the following error: RuntimeError('Input type (double) and bias type (float) should be the same').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_91719/3262429375.py\", line 15, in experiment\n",
      "    model.train(learning_rate, batch_size, 500)\n",
      "  File \"/tmp/ipykernel_91719/2048962979.py\", line 79, in train\n",
      "    self.train_decoder(verbose = verbose)\n",
      "  File \"/tmp/ipykernel_91719/2048962979.py\", line 76, in train_decoder\n",
      "    self.decoder_solver.fit(self.simple_train_dataloader, num_steps = self.steps)\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/utils.py\", line 180, in fit\n",
      "    self.step(batch)\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/utils.py\", line 196, in step\n",
      "    prediction = self._inference(X.float())\n",
      "                 ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/utils.py\", line 205, in _inference\n",
      "    prediction = self.model(X.float())\n",
      "                 ^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/cebra/models/model.py\", line 240, in forward\n",
      "    return self.net(inp)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 308, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 304, in _conv_forward\n",
      "    return F.conv1d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/utils/_device.py\", line 79, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (double) and bias type (float) should be the same\n",
      "[W 2024-11-29 15:20:34,995] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///data/optuna.db\u001b[39m\u001b[38;5;124m\"\u001b[39m, study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcebra_offsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[35], line 15\u001b[0m, in \u001b[0;36mexperiment\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m256\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelPipeline(\n\u001b[1;32m      7\u001b[0m     model_name,\n\u001b[1;32m      8\u001b[0m     X_train_tensor\u001b[38;5;241m=\u001b[39mX_train_tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     latent_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mscore()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Save the best model to disk\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 79\u001b[0m, in \u001b[0;36mModelPipeline.train\u001b[0;34m(self, learning_rate, batch_size, steps, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate, batch_size, steps, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_embedding(learning_rate, batch_size, steps, verbose)\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 76\u001b[0m, in \u001b[0;36mModelPipeline.train_decoder\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinaryClassifier \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m     65\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dimension,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dimension),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#Logit output\u001b[39;00m\n\u001b[1;32m     70\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_solver \u001b[38;5;241m=\u001b[39m SupervisedNNSolver(\n\u001b[1;32m     72\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinaryClassifier,\n\u001b[1;32m     73\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(),\n\u001b[1;32m     74\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinaryClassifier\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[1;32m     75\u001b[0m )\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_train_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/utils.py:180\u001b[0m, in \u001b[0;36mSupervisedNNSolver.fit\u001b[0;34m(self, loader, num_steps)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m         step_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_steps:\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/utils.py:196\u001b[0m, in \u001b[0;36mSupervisedNNSolver.step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    195\u001b[0m X, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m--> 196\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(prediction, y)\n\u001b[1;32m    198\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/utils.py:205\u001b[0m, in \u001b[0;36mSupervisedNNSolver._inference\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute predictions for the batch.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/cebra/models/model.py:240\u001b[0m, in \u001b[0;36m_OffsetModel.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the embedding given the input signal.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    is normalized to the hypersphere (`normalize = True`).\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:308\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/nn/modules/conv.py:304\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    302\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    303\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(storage=\"sqlite:///data/optuna.db\", study_name=\"cebra_offsets\", direction=\"minimize\", load_if_exists=True)\n",
    "study.optimize(experiment, n_trials=100)\n",
    "\n",
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
