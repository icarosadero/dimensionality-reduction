{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: cebra in /opt/conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.11/site-packages (3.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.0.2)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.11/site-packages (1.15.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: umap_learn in /opt/conda/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.11/site-packages (3.5.4)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.11/site-packages (2.6.2.2)\n",
      "Requirement already satisfied: optuna-dashboard in ./.local/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: duckdb-engine in ./.local/lib/python3.11/site-packages (0.14.2)\n",
      "Requirement already satisfied: PyMySQL in ./.local/lib/python3.11/site-packages (1.1.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.12.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.22)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.66.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from cebra) (1.4.2)\n",
      "Requirement already satisfied: literate-dataclasses in /opt/conda/lib/python3.11/site-packages (from cebra) (0.0.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from cebra) (1.6.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from cebra) (2.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from cebra) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.11/site-packages (from umap_learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.11/site-packages (from umap_learn) (0.5.13)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.11/site-packages (from tensorboardX) (5.29.2)\n",
      "Requirement already satisfied: bottle>=0.13.0 in /opt/conda/lib/python3.11/site-packages (from optuna-dashboard) (0.13.2)\n",
      "Requirement already satisfied: duckdb>=0.5.0 in ./.local/lib/python3.11/site-packages (from duckdb-engine) (1.1.3)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.2->umap_learn) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->cebra) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->cebra) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->cebra) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->cebra) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->cebra) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->cebra) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->cebra) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->cebra) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --user --upgrade optuna cebra matplotlib numpy pandas scipy seaborn umap_learn pyspark python-dotenv tensorboardX optuna-dashboard duckdb-engine PyMySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/main/external/dimensionality-reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cebra import CEBRA\n",
    "import torch\n",
    "import torch.utils\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.overrides import transform\n",
    "from utils.utils import pandas_series_to_pytorch, Decoder, Dataset\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import dotenv\n",
    "import os\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "dotenv.load_dotenv(\"/main/external/dimensionality-reduction/.env\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(\"/main/external/tensorboard_runs\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/10 14:20:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/10 14:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/01/10 14:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/01/10 14:20:16 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '120g'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.startTime', '1736518815661'), ('spark.executor.memory', '120g'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1736518816110'), ('spark.app.name', 'UranusCluster'), ('spark.memory.offHeap.enabled', 'true'), ('spark.app.submitTime', '1736518815581'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.driver.host', 'a9553030c10f'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.driver.port', '43407'), ('spark.memory.offHeap.size', '16g')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "MAX_MEMORY = \"120g\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"UranusCluster\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\")   \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = \"ID18170/DataFrame_Imaging_dFF_18170_day4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+\n",
      "|      index|         neural_data| positional_encoding|           file_name|          velocity|       acceleration|                 dt|\n",
      "+-----------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+\n",
      "|51539695482|[-0.0095341661944...|[-0.9938960058179...|ID18170/DataFrame...|               0.0|                0.0|                0.0|\n",
      "|51539695483|[-0.0045404555276...|[-0.9954778664531...|ID18170/DataFrame...|28.256165473888373| -904.4337054248496|0.03124183155095408|\n",
      "|51539695484|[0.05658219009637...|[-0.9968232372491...|ID18170/DataFrame...|28.253022407649297|0.10060441667608108|0.03124183155095407|\n",
      "|51539695485|[-0.0102527663111...|[-0.9979180828244...|ID18170/DataFrame...|27.856996061747097| 12.676156494099851|0.03124183155095408|\n",
      "|51539695486|[0.01038869749754...|[-0.9987675465275...|ID18170/DataFrame...|27.297530271494587| 17.907586158642637|0.03124183155095407|\n",
      "+-----------+--------------------+--------------------+--------------------+------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/main/external/data/transformed\")\\\n",
    "    .select([\"index\",\"neural_data\", \"positional_encoding\", \"file_name\", \"velocity\", \"acceleration\", \"dt\"])\\\n",
    "    .where(F.col(\"file_name\") == EXPERIMENT_ID)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a metric to determine a threshold to decide what rows correspond to the animal being completely at rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.911154442857518"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.select(F.percentile(F.col(\"velocity\"), 0.50), F.percentile(F.col(\"acceleration\"), 0.50)).collect()\n",
    "dt = df.select(F.mean(F.col(\"dt\"))).collect()[0][0]\n",
    "speed_median = row[0][0]\n",
    "acceleration_median = row[0][1]\n",
    "speed_acc_max = abs(speed_median) + abs(acceleration_median)*abs(dt)\n",
    "speed_acc_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512 #Used for both embedding and decoder\n",
    "test_ratio = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = df.select('index').rdd.flatMap(lambda x: x).collect()\n",
    "files = [x.file_name for x in df.select(\"file_name\").distinct().collect()]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/main/external/models'):\n",
    "    os.makedirs('/main/external/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "def objective(trial, file):\n",
    "    learning_rate_embedding = trial.suggest_float('learning_rate_embedding', 1e-5, 0.1, log = True)\n",
    "    learning_rate_decoder = trial.suggest_float('learning_rate_decoder', 1e-5, 0.1, log = True)\n",
    "    latent_dimension = trial.suggest_int('latent_dimension', 3, 100)\n",
    "    decoder_epochs = trial.suggest_int('decoder_epochs', 10, 100)\n",
    "    time_offsets = trial.suggest_int('time_offsets', 5, 100)\n",
    "    num_hidden_units = trial.suggest_int('num_hidden_units', 2,100)\n",
    "    speed_acc_filter = trial.suggest_float('speed_acc_filter', 0.0, speed_acc_max)\n",
    "    embedding_version = trial.suggest_categorical('model',choices=[\n",
    "        'offset1-model',\n",
    "        'offset1-model-v2',\n",
    "        'offset1-model-v3',\n",
    "        'offset1-model-v4',\n",
    "        'offset1-model-v5',\n",
    "        'offset5-model',\n",
    "        'offset10-model'\n",
    "    ])\n",
    "    \n",
    "    logging.info(f\"Started training on file {file}\")\n",
    "    \n",
    "    logging.debug(\"Trying to load dataframe into memory\")\n",
    "    du = df.where(F.col(\"file_name\") == file)\n",
    "    n_samples = du.count()\n",
    "    logging.debug(f\"There are {n_samples}\")\n",
    "    n_test = int(test_ratio * n_samples)\n",
    "    \n",
    "    du_train = du.select(F.col(\"neural_data\"), F.col(\"positional_encoding\"))\\\n",
    "        .orderBy(F.col(\"index\").asc()).limit(n_samples - n_test)\\\n",
    "        .where(F.abs(F.col(\"velocity\")) + F.abs(F.col(\"acceleration\")) * F.abs(F.col(\"dt\")) > speed_acc_filter)\\\n",
    "        .toPandas()\n",
    "    \n",
    "    du_test = du.select(F.col(\"neural_data\"), F.col(\"positional_encoding\"))\\\n",
    "        .orderBy(F.col(\"index\").asc()).limit(n_test)\\\n",
    "        .where(F.abs(F.col(\"velocity\")) + F.abs(F.col(\"acceleration\")) * F.abs(F.col(\"dt\")) > speed_acc_filter)\\\n",
    "        .toPandas()\n",
    "    \n",
    "    \n",
    "    with device:\n",
    "        X_test = pandas_series_to_pytorch(du_test.neural_data, device)\n",
    "        y_test = pandas_series_to_pytorch(du_test.positional_encoding, device)\n",
    "        X_train = pandas_series_to_pytorch(du_train.neural_data, device)\n",
    "        y_train = pandas_series_to_pytorch(du_train.positional_encoding, device)\n",
    "    \n",
    "    #Train embedding\n",
    "    logging.info(\"Training embedding\")\n",
    "    embedding = CEBRA(\n",
    "        model_architecture=embedding_version,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate_embedding,\n",
    "        temperature_mode='auto',\n",
    "        output_dimension=latent_dimension,\n",
    "        max_iterations=10000,\n",
    "        min_temperature=0.001,\n",
    "        distance='cosine',\n",
    "        conditional='time_delta',\n",
    "        device=str(device),\n",
    "        verbose=False,\n",
    "        time_offsets=time_offsets,\n",
    "        num_hidden_units=num_hidden_units\n",
    "    )\n",
    "    embedding.fit(X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    \n",
    "    logging.info(\"Training decoder\")\n",
    "    decoder = Decoder(latent_dimension).to(device)\n",
    "    with device:\n",
    "        # Train Decoder\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate_decoder)\n",
    "        for _ in range(decoder_epochs):\n",
    "            for i, (X_batch, y_batch) in enumerate(DataLoader(Dataset(X_train,y_train), batch_size=batch_size, shuffle=False)):\n",
    "                decoder.train()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                U = torch.Tensor(transform(embedding,X_batch.detach().cpu().numpy())).to(device)\n",
    "                y_pred = decoder(U)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                decoder_optimizer.step()\n",
    "                writer.add_scalar(f\"{file}/decoder/train\", loss.item(), i)\n",
    "    \n",
    "    U = torch.Tensor(transform(embedding, X_test.detach().cpu().numpy())).to(device)\n",
    "    total_loss = criterion(decoder(U), y_test)\n",
    "        \n",
    "    #Calculating metric\n",
    "    if (hasattr(objective, \"best_loss\") and total_loss < objective.best_loss) or not hasattr(objective, \"best_loss\"):\n",
    "        objective.best_loss = total_loss\n",
    "\n",
    "        logging.info(\"Saving models\")\n",
    "        savepath = os.path.join(\"/main/external/models\", file)\n",
    "        if not os.path.exists(savepath):\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "        embedding.save(os.path.join(savepath, \"embedding.pt\"))\n",
    "        torch.save(decoder, os.path.join(savepath, \"decoder.pt\"))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][I 2025-01-10 14:20:52,158] A new study created in RDB with name: ID18170/DataFrame_Imaging_dFF_18170_day4_1736518851978\n",
      "2025-01-10 14:20:52 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:20:52 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:21:09 [INFO] Training embedding                                   \n",
      "2025-01-10 14:21:09 [INFO] Training embedding\n",
      "2025-01-10 14:21:55 [INFO] Training decoder\n",
      "2025-01-10 14:21:55 [INFO] Training decoder\n",
      "2025-01-10 14:22:11 [INFO] Saving models\n",
      "[I 2025-01-10 14:22:11,327] Trial 0 finished with value: 18936.123046875 and parameters: {'learning_rate_embedding': 0.010903482112798533, 'learning_rate_decoder': 2.3096961503576414e-05, 'latent_dimension': 79, 'decoder_epochs': 24, 'time_offsets': 28, 'num_hidden_units': 91, 'speed_acc_filter': 3.361114591898735, 'model': 'offset1-model-v3'}. Best is trial 0 with value: 18936.123046875.\n",
      "2025-01-10 14:22:11 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:22:25 [INFO] Training embedding                                   \n",
      "2025-01-10 14:22:26 [INFO] Saving models\n",
      "[I 2025-01-10 14:22:26,507] Trial 1 finished with value: 177.72457885742188 and parameters: {'learning_rate_embedding': 0.001178141027302991, 'learning_rate_decoder': 0.0011891350083846407, 'latent_dimension': 55, 'decoder_epochs': 67, 'time_offsets': 67, 'num_hidden_units': 76, 'speed_acc_filter': 5.611416991468808, 'model': 'offset1-model-v4'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:22:26 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:22:39 [INFO] Training embedding                                   \n",
      "2025-01-10 14:23:05 [INFO] Training decoder\n",
      "[I 2025-01-10 14:23:27,861] Trial 2 finished with value: 194.58839416503906 and parameters: {'learning_rate_embedding': 1.6268622733748473e-05, 'learning_rate_decoder': 0.004715896390392651, 'latent_dimension': 71, 'decoder_epochs': 50, 'time_offsets': 60, 'num_hidden_units': 17, 'speed_acc_filter': 9.662731584111583, 'model': 'offset5-model'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:23:27 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:23:30 [INFO] Training decoder====================>  (61 + 3) / 64]\n",
      "2025-01-10 14:23:42 [INFO] Training embedding                                   \n",
      "[I 2025-01-10 14:24:01,724] Trial 3 finished with value: 16024.94140625 and parameters: {'learning_rate_embedding': 0.0009346151234603717, 'learning_rate_decoder': 0.07222290793835426, 'latent_dimension': 94, 'decoder_epochs': 73, 'time_offsets': 30, 'num_hidden_units': 67, 'speed_acc_filter': 9.142556640689966, 'model': 'offset5-model'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:24:01 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:24:15 [INFO] Training embedding                                   \n",
      "2025-01-10 14:24:21 [INFO] Training decoder\n",
      "[I 2025-01-10 14:24:34,517] Trial 4 finished with value: 27629.611328125 and parameters: {'learning_rate_embedding': 0.037355816321724335, 'learning_rate_decoder': 3.9375681115227846e-05, 'latent_dimension': 49, 'decoder_epochs': 36, 'time_offsets': 13, 'num_hidden_units': 78, 'speed_acc_filter': 9.6849381904582, 'model': 'offset1-model-v2'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:24:34 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:24:48 [INFO] Training embedding                                   \n",
      "2025-01-10 14:25:11 [INFO] Training decoder\n",
      "[I 2025-01-10 14:25:45,088] Trial 5 finished with value: 11725.541015625 and parameters: {'learning_rate_embedding': 0.001494210625348545, 'learning_rate_decoder': 1.3880540411340232e-05, 'latent_dimension': 99, 'decoder_epochs': 91, 'time_offsets': 97, 'num_hidden_units': 42, 'speed_acc_filter': 5.99696699149514, 'model': 'offset1-model-v5'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:25:45 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:25:50 [INFO] Training decoder                         (0 + 1) / 1]\n",
      "2025-01-10 14:25:58 [INFO] Training embedding                                   \n",
      "[I 2025-01-10 14:26:12,227] Trial 6 finished with value: 34768.8828125 and parameters: {'learning_rate_embedding': 0.00046532536155308613, 'learning_rate_decoder': 2.2924187041912313e-05, 'latent_dimension': 9, 'decoder_epochs': 50, 'time_offsets': 45, 'num_hidden_units': 12, 'speed_acc_filter': 2.198232899582693, 'model': 'offset1-model-v3'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:26:12 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:26:25 [INFO] Training embedding                                   \n",
      "2025-01-10 14:26:35 [INFO] Training decoder\n",
      "[I 2025-01-10 14:26:45,586] Trial 7 finished with value: 245.004150390625 and parameters: {'learning_rate_embedding': 0.022053370475500534, 'learning_rate_decoder': 0.0002780986468758011, 'latent_dimension': 59, 'decoder_epochs': 26, 'time_offsets': 20, 'num_hidden_units': 21, 'speed_acc_filter': 10.64476626876322, 'model': 'offset1-model'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:26:45 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:26:59 [INFO] Training embedding                                   \n",
      "2025-01-10 14:27:04 [INFO] Training decoder\n",
      "[I 2025-01-10 14:27:20,081] Trial 8 finished with value: 530.4368896484375 and parameters: {'learning_rate_embedding': 0.004456982294201499, 'learning_rate_decoder': 0.07744463684978854, 'latent_dimension': 43, 'decoder_epochs': 36, 'time_offsets': 91, 'num_hidden_units': 39, 'speed_acc_filter': 8.657078427501203, 'model': 'offset1-model-v2'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:27:20 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:27:33 [INFO] Training embedding                                   \n",
      "2025-01-10 14:27:37 [INFO] Training decoder\n",
      "[I 2025-01-10 14:28:03,744] Trial 9 finished with value: 212.39260864257812 and parameters: {'learning_rate_embedding': 0.00013296983600800098, 'learning_rate_decoder': 0.002278868095005607, 'latent_dimension': 81, 'decoder_epochs': 69, 'time_offsets': 82, 'num_hidden_units': 15, 'speed_acc_filter': 7.025991128675293, 'model': 'offset1-model'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:28:03 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:28:18 [INFO] Training embedding                                   \n",
      "2025-01-10 14:28:35 [INFO] Training decoder\n",
      "[I 2025-01-10 14:29:20,354] Trial 10 finished with value: 220.78759765625 and parameters: {'learning_rate_embedding': 0.00018167990898964033, 'learning_rate_decoder': 0.017992025048485835, 'latent_dimension': 16, 'decoder_epochs': 84, 'time_offsets': 6, 'num_hidden_units': 41, 'speed_acc_filter': 1.7553591747888044, 'model': 'offset5-model'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:29:20 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:29:25 [INFO] Training decoder======================>(63 + 1) / 64]\n",
      "2025-01-10 14:29:34 [INFO] Training embedding                                   \n",
      "[I 2025-01-10 14:30:08,657] Trial 11 finished with value: 178.61767578125 and parameters: {'learning_rate_embedding': 9.722888745053538e-05, 'learning_rate_decoder': 0.0003443050842802051, 'latent_dimension': 27, 'decoder_epochs': 93, 'time_offsets': 68, 'num_hidden_units': 100, 'speed_acc_filter': 0.41515354675331473, 'model': 'offset1-model-v4'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:30:08 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:30:22 [INFO] Training embedding                                   \n",
      "2025-01-10 14:30:54 [INFO] Training decoder\n",
      "2025-01-10 14:31:22 [INFO] Training decoder\n",
      "[I 2025-01-10 14:31:25,810] Trial 12 finished with value: 368.79681396484375 and parameters: {'learning_rate_embedding': 1.0470210869840538e-05, 'learning_rate_decoder': 0.0022601946972066298, 'latent_dimension': 67, 'decoder_epochs': 57, 'time_offsets': 67, 'num_hidden_units': 63, 'speed_acc_filter': 3.9149935446211934, 'model': 'offset1-model-v4'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:31:25 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:31:40 [INFO] Training embedding                                   \n",
      "[I 2025-01-10 14:32:04,140] Trial 13 finished with value: 195.71673583984375 and parameters: {'learning_rate_embedding': 2.9677066438087125e-05, 'learning_rate_decoder': 0.00027717074468266284, 'latent_dimension': 28, 'decoder_epochs': 97, 'time_offsets': 69, 'num_hidden_units': 100, 'speed_acc_filter': 3.9528562219041015, 'model': 'offset1-model-v4'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:32:04 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:32:18 [INFO] Training embedding                                   \n",
      "2025-01-10 14:32:52 [INFO] Training decoder\n",
      "2025-01-10 14:33:15 [INFO] Training decoder\n",
      "[I 2025-01-10 14:33:58,544] Trial 14 finished with value: 1006.0789184570312 and parameters: {'learning_rate_embedding': 6.726872151592762e-05, 'learning_rate_decoder': 0.00020833258663101697, 'latent_dimension': 30, 'decoder_epochs': 97, 'time_offsets': 69, 'num_hidden_units': 99, 'speed_acc_filter': 0.2508753745478753, 'model': 'offset1-model-v4'}. Best is trial 1 with value: 177.72457885742188.\n",
      "2025-01-10 14:33:58 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:34:06 [INFO] Saving models=======================>  (61 + 3) / 64]\n",
      "[I 2025-01-10 14:34:09,484] Trial 15 finished with value: 144.34133911132812 and parameters: {'learning_rate_embedding': 8.697442963668999e-05, 'learning_rate_decoder': 0.0003144054968600056, 'latent_dimension': 34, 'decoder_epochs': 78, 'time_offsets': 46, 'num_hidden_units': 83, 'speed_acc_filter': 0.45314393618664345, 'model': 'offset1-model-v4'}. Best is trial 15 with value: 144.34133911132812.\n",
      "2025-01-10 14:34:09 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2025-01-10 14:34:13 [INFO] Training embedding                                   \n",
      "2025-01-10 14:34:21 [INFO] Training embedding                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43372)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    objective_ = partial(objective, file = file)\n",
    "    study = optuna.create_study(\n",
    "        storage = \"mysql+pymysql://root:password@131.220.127.56/optuna\",\n",
    "        load_if_exists=True,\n",
    "        study_name=file + \"_\" + str(int(round(time.time() * 1000))),\n",
    "        direction=\"minimize\"\n",
    "    )\n",
    "    study.optimize(objective_, n_trials=100, n_jobs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
