{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade optuna cebra>=0.4.0 matplotlib==3.9.2 numpy pandas scipy seaborn umap_learn pyspark python-dotenv tensorboardX optuna-dashboard duckdb-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/main/external/dimensionality-reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cebra import CEBRA\n",
    "import torch\n",
    "import torch.utils\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils.overrides import transform\n",
    "from utils.utils import pandas_series_to_pytorch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "import dotenv\n",
    "import os\n",
    "import time\n",
    "dotenv.load_dotenv()\n",
    "dotenv.load_dotenv(\"/main/external/dimensionality-reduction/.env\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(\"/main/external/tensorboard_runs\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/23 09:45:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '120g'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.submitTime', '1734947107366'), ('spark.executor.memory', '120g'), ('spark.executor.id', 'driver'), ('spark.app.name', 'UranusCluster'), ('spark.memory.offHeap.enabled', 'true'), ('spark.app.startTime', '1734947107421'), ('spark.driver.host', 'f17484681f9b'), ('spark.app.id', 'local-1734947107872'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.memory.offHeap.size', '16g'), ('spark.driver.port', '41539')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "MAX_MEMORY = \"120g\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"UranusCluster\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\")   \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_ID = \"ID18170/DataFrame_Imaging_dFF_18170_day4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|      index|         neural_data| positional_encoding|           file_name|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|51539752512|[0.57670706510543...|-0.01445482831236...|ID18170/DataFrame...|\n",
      "|51539695187|[-0.0095341661944...|-0.00437369369180...|ID18170/DataFrame...|\n",
      "|51539752513|[0.54644668102264...|-0.01685933642369...|ID18170/DataFrame...|\n",
      "|51539695188|[-0.0045404555276...|0.010276542098878195|ID18170/DataFrame...|\n",
      "|51539752514|[0.51589649915695...|-0.00761060092563...|ID18170/DataFrame...|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/main/external/data/transformed\")\\\n",
    "    .select([\"index\",\"neural_data\", \"positional_encoding\", \"file_name\"])\\\n",
    "    .where(F.col(\"file_name\") == EXPERIMENT_ID)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_partitions = 32\n",
    "batch_size = 128 #Used for both embdding and decoder\n",
    "n_splits = 16\n",
    "latent_dimension = 3\n",
    "test_ratio = 0.3\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = df.select('index').rdd.flatMap(lambda x: x).collect()\n",
    "files = [x.file_name for x in df.select(\"file_name\").distinct().collect()]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('/main/external/models'):\n",
    "    os.makedirs('/main/external/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, file):\n",
    "    learning_rate_embedding = trial.suggest_float('learning_rate_embedding', 1e-10, 0.1, log = True)\n",
    "    learning_rate_decoder = trial.suggest_float('learning_rate_decoder', 1e-10, 0.1, log = True)\n",
    "    embedding_version = trial.suggest_categorical('model',choices=[\n",
    "        'offset1-model',\n",
    "        'offset1-model-v1',\n",
    "        'offset1-model-v2',\n",
    "        'offset1-model-v3',\n",
    "        'offset1-model-v4',\n",
    "        'offset1-model-v5'\n",
    "    ])\n",
    "    \n",
    "    logging.info(f\"Started training on file {file}\")\n",
    "    decoder = torch.nn.Sequential(\n",
    "        torch.nn.Linear(latent_dimension, 3),\n",
    "        torch.nn.GELU(),\n",
    "        torch.nn.Linear(3,3),\n",
    "        torch.nn.GELU(),\n",
    "        torch.nn.Linear(3,1),\n",
    "        torch.nn.Tanh()\n",
    "    ).to(device)\n",
    "    \n",
    "    logging.debug(\"Trying to load dataframe into memory\")\n",
    "    du = df.where(F.col('file_name') == file).toPandas()\n",
    "    \n",
    "    n_samples = du.shape[0]\n",
    "    logging.debug(f\"There are {n_samples}\")\n",
    "    n_test = int(test_ratio * n_samples)\n",
    "    \n",
    "    with device:\n",
    "        X_test = pandas_series_to_pytorch(du.neural_data[-n_test:], device)\n",
    "        y_test = pandas_series_to_pytorch(du.positional_encoding[-n_test:], device)\n",
    "        X_train = pandas_series_to_pytorch(du.neural_data[:n_test], device)\n",
    "        y_train = pandas_series_to_pytorch(du.positional_encoding[:n_test], device)\n",
    "    \n",
    "    #Train embedding\n",
    "    logging.info(\"Training embedding\")\n",
    "    embedding = CEBRA(\n",
    "        model_architecture=embedding_version,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate_embedding,\n",
    "        temperature_mode='auto',\n",
    "        output_dimension=latent_dimension,\n",
    "        max_iterations=50000,\n",
    "        min_temperature=0.001,\n",
    "        distance='cosine',\n",
    "        conditional='time_delta',\n",
    "        device=str(device),\n",
    "        verbose=False,\n",
    "        time_offsets=10\n",
    "    )\n",
    "    embedding.fit(X_train.detach().cpu().numpy(), y_train.detach().cpu().numpy())\n",
    "    \n",
    "    logging.info(\"Training decoder\")\n",
    "    with device:\n",
    "        # Train Decoder\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate_decoder)\n",
    "        for i, (X_batch, y_batch) in enumerate(DataLoader(Dataset(X_train,y_train), batch_size=batch_size, shuffle=False)):\n",
    "            decoder.train()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            U = torch.Tensor(transform(embedding,X_batch.detach().cpu().numpy())).to(device)\n",
    "            y_pred = decoder(U)\n",
    "            loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            decoder_optimizer.step()\n",
    "            writer.add_scalar(f\"{file}/decoder/train\", loss.item(), i)\n",
    "        \n",
    "            # Test Decoder\n",
    "            losses = []\n",
    "            for (X_batch, y_batch) in DataLoader(Dataset(X_test,y_test), batch_size=batch_size, shuffle=False):\n",
    "                decoder.eval()\n",
    "                U = torch.Tensor(transform(embedding,X_batch.detach().cpu().numpy())).to(device)\n",
    "                y_pred = decoder(U)\n",
    "                loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "                losses.append(loss.item())\n",
    "            writer.add_scalar(f\"{file}/decoder/test\", np.mean(losses), i)\n",
    "\n",
    "        logging.info(\"Logging reconstruction\")\n",
    "        # Reconstruction comparison\n",
    "        counter = 0\n",
    "        total_loss = 0\n",
    "        for (X_batch, y_batch) in DataLoader(Dataset(X_test,y_test), batch_size=batch_size, shuffle=False):\n",
    "            decoder.eval()\n",
    "            U = torch.Tensor(transform(embedding,X_batch.detach().cpu().numpy())).to(device)\n",
    "            y_pred = decoder(U).flatten()\n",
    "            for y_true, y_pred in zip(y_batch, y_pred):\n",
    "                diff = y_true - y_pred\n",
    "                total_loss += diff*diff\n",
    "                writer.add_scalar(f\"{file}/series/prediction_difference_relative\", diff/y_true, i)\n",
    "                i += 1\n",
    "        \n",
    "    #Calculating metric\n",
    "    if (hasattr(objective, \"best_loss\") and total_loss < objective.best_loss) or not hasattr(objective, \"best_loss\"):\n",
    "        objective.best_loss = total_loss\n",
    "\n",
    "        logging.info(\"Saving models\")\n",
    "        savepath = os.path.join(\"/main/external/models\", file)\n",
    "        if not os.path.exists(savepath):\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "        embedding.save(os.path.join(savepath, \"embedding.pt\"))\n",
    "        torch.save(decoder, os.path.join(savepath, \"decoder.pt\"))\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s][I 2024-12-23 09:51:40,173] A new study created in RDB with name: ID18170/DataFrame_Imaging_dFF_18170_day4_1734947500160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 09:51:40 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2024-12-23 09:51:50 [INFO] Training embedding                                   \n",
      "2024-12-23 09:53:18 [INFO] Training decoder\n",
      "2024-12-23 09:54:04 [INFO] Logging reconstruction\n",
      "2024-12-23 09:54:08 [INFO] Saving models\n",
      "[I 2024-12-23 09:54:08,654] Trial 0 finished with value: 476.3794250488281 and parameters: {'learning_rate_embedding': 6.3391233170425225e-06, 'learning_rate_decoder': 2.2964145483889253e-08, 'model': 'offset1-model'}. Best is trial 0 with value: 476.3794250488281.\n",
      "2024-12-23 09:54:08 [INFO] Started training on file ID18170/DataFrame_Imaging_dFF_18170_day4\n",
      "2024-12-23 09:54:18 [INFO] Training embedding                                   \n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    objective_ = partial(objective, file = file)\n",
    "    study = optuna.create_study(\n",
    "        storage = \"sqlite:///optuna.db\",\n",
    "        load_if_exists=True,\n",
    "        study_name=file + \"_\" + str(int(round(time.time() * 1000))),\n",
    "        direction=\"minimize\"\n",
    "    )\n",
    "    study.optimize(objective_, n_trials=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
