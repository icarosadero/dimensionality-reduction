{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.11/site-packages (4.1.0)\n",
      "Requirement already satisfied: cebra==0.4.0 in /opt/conda/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: matplotlib==3.9.2 in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (1.14.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: umap_learn in /opt/conda/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.11/site-packages (3.5.3)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (1.4.2)\n",
      "Requirement already satisfied: literate-dataclasses in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (0.0.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (1.6.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (4.66.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from cebra==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib==3.9.2) (2.8.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.12.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.11/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.22)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.11/site-packages (from umap_learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.11/site-packages (from umap_learn) (0.5.13)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (4.8.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.2->umap_learn) (0.43.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib==3.9.2) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->cebra==0.4.0) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->cebra==0.4.0) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->cebra==0.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->cebra==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->cebra==0.4.0) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->cebra==0.4.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->cebra==0.4.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->cebra==0.4.0) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna cebra==0.4.0 matplotlib==3.9.2 numpy pandas scipy seaborn umap_learn pyspark python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/main/external/dimensionality-reduction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cebra\n",
    "import torch\n",
    "import torch.utils\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from utils import transform\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "dotenv.load_dotenv(\"/main/external/dimensionality-reduction/.env\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/17 14:54:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.memory', '120g'), ('spark.app.id', 'local-1734447249800'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.submitTime', '1734447249257'), ('spark.app.name', 'Foo'), ('spark.executor.memory', '120g'), ('spark.executor.id', 'driver'), ('spark.memory.offHeap.enabled', 'true'), ('spark.driver.host', '0034c8293b66'), ('spark.rdd.compress', 'True'), ('spark.app.startTime', '1734447249340'), ('spark.driver.port', '34905'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.memory.offHeap.size', '16g')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "MAX_MEMORY = \"120g\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"UranusCluster\") \\\n",
    "    .appName(\"Foo\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",True)\\\n",
    "    .config(\"spark.memory.offHeap.size\",\"16g\")   \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verify the SparkContext\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking just one because this still doesn`t support multi session data\n",
    "DATA_PATH = \"ID18150/Day6/DataFrame_Imaging_spiking_18150_day6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+\n",
      "|       index|         neural_data| positional_encoding|           file_name|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "|317827579904|[0.00353814595291...|0.014736815295795708|ID18150/Day6/Data...|\n",
      "|317827707084|[7.32697662897408...|0.002792727523517...|ID18150/Day6/Data...|\n",
      "|317827579905|[0.00376488942492...|0.014736815295795708|ID18150/Day6/Data...|\n",
      "|317827707085|[5.27972821146249...|0.002792727523517...|ID18150/Day6/Data...|\n",
      "|317827579906|[0.00368517413153...| 0.01473313965622418|ID18150/Day6/Data...|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"/main/external/data/transformed\")\\\n",
    "    .select([\"index\",\"neural_data\", \"positional_encoding\", \"file_name\"])\\\n",
    "    .where(F.col(\"file_name\") == DATA_PATH)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(131777, 317827711680, 317827579904)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_neurons = max([row.neural_size for row in df.select(F.max(F.size(\"neural_data\")).alias(\"neural_size\")).collect()])\n",
    "# n_rows = df.count()\n",
    "indices = df.select('index').rdd.flatMap(lambda x: x).collect()\n",
    "len(indices), max(indices), min(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, p):\n",
    "    d = len(X) // p\n",
    "    X_reduced = X[:(d * p)]\n",
    "    partitions = np.split(np.array(X_reduced), p)\n",
    "    partitions = [a.tolist() for a in partitions]\n",
    "    if len(X[d*p:]) != 0:\n",
    "        partitions += [X[d*p:]]\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 5000\n",
    "def get_x_ticks(L:int):\n",
    "    x_ticks = np.arange(0,L,delta)\n",
    "    x_tick_labels = [f\"{t[i]/100:.2f}\" for i in x_ticks]\n",
    "    return x_ticks, x_tick_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEBRA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_size = 512\n",
    "kf = KFold(n_splits = 4)\n",
    "latent_dimension = 8\n",
    "scores = []\n",
    "\n",
    "from cebra import CEBRA\n",
    "\n",
    "multi_cebra_model = CEBRA(model_architecture='offset1-model',\n",
    "                    batch_size=64,\n",
    "                    learning_rate=3e-4,\n",
    "                    temperature_mode='auto',\n",
    "                    output_dimension=latent_dimension,\n",
    "                    max_iterations=1000,\n",
    "                    distance='cosine',\n",
    "                    conditional='time_delta',\n",
    "                    device='cuda_if_available',\n",
    "                    verbose=True,\n",
    "                    time_offsets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = torch.nn.Sequential(\n",
    "    torch.nn.Linear(latent_dimension, 3),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(3,3),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(3,1),\n",
    "    torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:10,685 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:10,686 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:10,708 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 211:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:17,377 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -1.1442 neg:  3.8109 total:  2.6667 temperature:  0.5114: 100%|██████████| 1000/1000 [00:02<00:00, 364.14it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:20,128 - STDOUT - INFO - Average score: -234.38885498046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -234.38885498046875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:20,128 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:20,129 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:20,150 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 241:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:26,824 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -1.3127 neg:  4.0164 total:  2.7037 temperature:  0.4490: 100%|██████████| 1000/1000 [00:02<00:00, 395.37it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:29,359 - STDOUT - INFO - Average score: -237.61734008789062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -237.61734008789062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:29,359 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:29,359 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:29,379 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 271:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:36,076 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -1.5214 neg:  4.1707 total:  2.6493 temperature:  0.3971: 100%|██████████| 1000/1000 [00:02<00:00, 397.34it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:38,601 - STDOUT - INFO - Average score: -226.2185516357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -226.2185516357422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:38,602 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:38,602 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:38,620 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 301:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:45,351 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -1.7819 neg:  4.2536 total:  2.4717 temperature:  0.3554: 100%|██████████| 1000/1000 [00:02<00:00, 373.73it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:48,031 - STDOUT - INFO - Average score: -253.38893127441406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -253.38893127441406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:48,032 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:48,032 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:48,054 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 331:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:54,749 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -2.2554 neg:  4.5425 total:  2.2872 temperature:  0.3186: 100%|██████████| 1000/1000 [00:02<00:00, 388.48it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:57,328 - STDOUT - INFO - Average score: -80.11901092529297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -80.11901092529297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:57,329 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:57,329 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:03:57,347 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 361:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:04,020 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -2.3257 neg:  4.6719 total:  2.3462 temperature:  0.2879: 100%|██████████| 1000/1000 [00:03<00:00, 327.01it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:07,084 - STDOUT - INFO - Average score: -89.7493896484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -89.7493896484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:07,085 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:07,085 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:07,105 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 391:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:13,799 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -3.2738 neg:  5.5297 total:  2.2559 temperature:  0.2631: 100%|██████████| 1000/1000 [00:03<00:00, 331.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:16,823 - STDOUT - INFO - Average score: -81.24207305908203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n",
      "INFO:STDOUT:Average score: -81.24207305908203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:16,824 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:16,824 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:16,843 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 421:======================================================>(63 + 1) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:23,528 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -2.8065 neg:  5.5940 total:  2.7876 temperature:  0.2427: 100%|██████████| 1000/1000 [00:02<00:00, 349.41it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:26,395 - STDOUT - INFO - Average score: -73.75111389160156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -73.75111389160156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:26,396 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:26,396 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:26,414 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 451:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:33,076 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -3.8285 neg:  5.9093 total:  2.0808 temperature:  0.2244: 100%|██████████| 1000/1000 [00:02<00:00, 406.05it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:35,543 - STDOUT - INFO - Average score: -71.16227722167969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -71.16227722167969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:35,544 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:35,544 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:35,563 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 481:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:42,244 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -3.8293 neg:  6.2521 total:  2.4228 temperature:  0.2100: 100%|██████████| 1000/1000 [00:03<00:00, 327.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:45,302 - STDOUT - INFO - Average score: -70.93733215332031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n",
      "INFO:STDOUT:Average score: -70.93733215332031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:45,302 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:45,303 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:45,321 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "[Stage 511:====================================================>  (61 + 3) / 64]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:51,993 - STDOUT - INFO - Performing partial fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Performing partial fits                                             \n",
      "pos: -3.6132 neg:  6.2063 total:  2.5931 temperature:  0.1963: 100%|██████████| 1000/1000 [00:03<00:00, 325.92it/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/_device.py:106: UserWarning: Using a target size (torch.Size([193])) that is different to the input size (torch.Size([193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:55,066 - STDOUT - INFO - Average score: -75.11351776123047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Average score: -75.11351776123047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:55,067 - STDOUT - INFO - Starting new iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Starting new iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:55,067 - STDOUT - INFO - Filtering dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Filtering dataframe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:04:55,087 - STDOUT - INFO - Grouping data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:STDOUT:Grouping data\n",
      "ERROR:root:KeyboardInterrupt while sending command.===========>   (60 + 4) / 64]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrouping data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m groups_train \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_train\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[0;32m---> 16\u001b[0m groups_list_train \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroups_train\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     17\u001b[0m groups_test \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_test\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[1;32m     18\u001b[0m groups_list_test \u001b[38;5;241m=\u001b[39m [df_test\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39mx)\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m groups_test]\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrouping data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m groups_train \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_train\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[0;32m---> 16\u001b[0m groups_list_train \u001b[38;5;241m=\u001b[39m [\u001b[43mdf_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m groups_train]\n\u001b[1;32m     17\u001b[0m groups_test \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m df_test\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39mcollect()]\n\u001b[1;32m     18\u001b[0m groups_list_test \u001b[38;5;241m=\u001b[39m [df_test\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m==\u001b[39mx)\u001b[38;5;241m.\u001b[39mcollect() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m groups_test]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n"
     ]
    }
   ],
   "source": [
    "scores_hist = []\n",
    "for train_indices, test_indices in kf.split(indices):\n",
    "    logging.info(f\"Started with {len(train_indices)=} and {len(test_indices)=}\")\n",
    "    for train_partition_indices, test_partition_indices in zip(split(train_indices, partition_size),split(test_indices, partition_size)):\n",
    "        logging.info(\"Starting new iteration\")\n",
    "        train_partition_indices = [[indices[i]] for i in train_partition_indices]\n",
    "        test_partition_indices  = [[indices[i]] for i in test_partition_indices]\n",
    "        \n",
    "        logging.info(\"Filtering dataframe\")\n",
    "        df_train_index = spark.createDataFrame(train_partition_indices[:], [\"index\"])\n",
    "        df_test_index  = spark.createDataFrame(test_partition_indices[:], [\"index\"])\n",
    "        df_train = df_train_index.join(df, how = \"left\", on = \"index\")\n",
    "        df_test  = df_test_index.join(df, how = \"left\", on = \"index\")\n",
    "        \n",
    "        logging.info(\"Grouping data\")\n",
    "        groups_train = [x[0] for x in df_train.select(\"file_name\").distinct().collect()]\n",
    "        groups_list_train = [df_train.filter(F.col(\"file_name\")==x).collect() for x in groups_train]\n",
    "        groups_test = [x[0] for x in df_test.select(\"file_name\").distinct().collect()]\n",
    "        groups_list_test = [df_test.filter(F.col(\"file_name\")==x).collect() for x in groups_test]\n",
    "        \n",
    "        X_train = [torch.Tensor(list(map(lambda x: x.neural_data, group))) for group in groups_list_train]\n",
    "        X_test = [torch.Tensor(list(map(lambda x: x.neural_data, group))) for group in groups_list_test]\n",
    "        y_train = [torch.Tensor(list(map(lambda x: x.positional_encoding, group))) for group in groups_list_train]\n",
    "        y_test = [torch.Tensor(list(map(lambda x: x.positional_encoding, group))) for group in groups_list_test]\n",
    "        \n",
    "        logging.info(\"Performing partial fits\")\n",
    "        for X,y in zip(X_train, y_train):\n",
    "            with device:\n",
    "                # Train Embedding\n",
    "                multi_cebra_model.partial_fit(X, y) #Partial fit doesn`t work on multi session data :(\n",
    "                embedding = torch.Tensor(transform(multi_cebra_model,X))\n",
    "                \n",
    "                # Train Decoder\n",
    "                decoder.train()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                predicted_embedding = decoder(embedding)\n",
    "                loss = criterion(predicted_embedding, y)\n",
    "                loss.backward()\n",
    "                decoder_optimizer.step()\n",
    "        \n",
    "        scores = []\n",
    "        for X,y in zip(X_test, y_test):\n",
    "            with device:\n",
    "                # Test\n",
    "                test_embedding = torch.Tensor(transform(multi_cebra_model, X))\n",
    "                decoder.eval()\n",
    "                pred = decoder(test_embedding)\n",
    "                score = r2_score(y.detach().numpy(), pred.detach().numpy())\n",
    "                scores.append(score)\n",
    "        mean_score = np.mean(scores)\n",
    "        \n",
    "        logging.info(f\"Average score: {mean_score}\")\n",
    "        \n",
    "        scores_hist.append(mean_score)\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
