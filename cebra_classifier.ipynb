{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icaro/Documents/doctorate/dimensionality-reduction/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cebra\n",
    "import torch\n",
    "import torch.utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import r2_score\n",
    "from utils import TensorDataset, SimpleTensorDataset, SupervisedNNSolver\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_hdf(\"data/ID18150/Day2/DataFrame_Imaging_dFF_18150_day2.h5\")\n",
    "# Get a list of columns whose names are of numerical type\n",
    "numerical_columns = [col for col in df.columns if type(col) == int]\n",
    "\n",
    "#Feature matrix\n",
    "X = df[numerical_columns].values\n",
    "t = df.Time.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 5000\n",
    "def get_x_ticks(L:int):\n",
    "    x_ticks = np.arange(0,L,delta)\n",
    "    x_tick_labels = [f\"{t[i]/100:.2f}\" for i in x_ticks]\n",
    "    return x_ticks, x_tick_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52796, 709), (13198, 709))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = int(0.2*len(X))\n",
    "X_train, X_test = X[:-split], X[-split:]\n",
    "y_train, y_test = df[\"Pump\"].values[:-split], df[\"Pump\"].values[-split:]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_ticks, x_tick_labels = get_x_ticks(X.shape[0])\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "# ax = sns.heatmap(X.T, ax=ax, cmap=\"gray_r\")\n",
    "# ax.set_xticks(x_ticks)\n",
    "# ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "# v_bar = X_train.shape[0]\n",
    "# ax.axvline(v_bar, color=\"red\")\n",
    "\n",
    "# ax.set_title(\"Train and Test split\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEBRA Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with device:\n",
    "    X_train_tensor = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "    X_test_tensor = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "    y_train_tensor = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "    y_test_tensor = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelPipeline:\n",
    "    def __init__(self, model_name, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, num_units, latent_dimension = 8):\n",
    "        self.model_name = model_name\n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        self.X_test_tensor = X_test_tensor\n",
    "        self.y_test_tensor = y_test_tensor\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.num_units = num_units\n",
    "        self.train_dataset = TensorDataset(\n",
    "            neural = X_train_tensor,\n",
    "            discrete = y_train_tensor,\n",
    "        )\n",
    "        self.test_dataset = TensorDataset(\n",
    "            neural = X_test_tensor,\n",
    "            discrete = y_test_tensor\n",
    "        )\n",
    "        with device:\n",
    "            self.model = cebra.models.init(\n",
    "                name = model_name,\n",
    "                num_neurons = self.train_dataset.neural.shape[1],\n",
    "                num_units = num_units,\n",
    "                num_output = latent_dimension\n",
    "            )\n",
    "            self.train_dataset.configure_for(self.model)\n",
    "            self.test_dataset.configure_for(self.model)\n",
    "    def train_embedding(self, learning_rate, batch_size, steps = 1000, verbose = True):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.steps = steps\n",
    "        with device:\n",
    "            criterion = cebra.models.criterions.LearnableCosineInfoNCE()\n",
    "            optimizer = torch.optim.Adam(\n",
    "                list(self.model.parameters()) + list(criterion.parameters()),\n",
    "                lr = learning_rate\n",
    "            )\n",
    "            self.embedding_solver = cebra.solver.SingleSessionSolver(\n",
    "                model = self.model,\n",
    "                criterion = criterion,\n",
    "                optimizer = optimizer,\n",
    "                tqdm_on = verbose\n",
    "            )\n",
    "            train_loader = cebra.data.single_session.DiscreteDataLoader(\n",
    "                dataset = self.train_dataset,\n",
    "                num_steps = steps,\n",
    "                batch_size = batch_size,\n",
    "                prior = \"empirical\"\n",
    "            )\n",
    "            self.embedding_solver.fit(loader=train_loader)\n",
    "    def train_decoder(self, verbose = True):\n",
    "        with device:\n",
    "            self.simple_train_dataloader = torch.utils.data.DataLoader(\n",
    "                SimpleTensorDataset(\n",
    "                    data = self.X_train_tensor.type(torch.FloatTensor),\n",
    "                    labels = self.y_train_tensor.type(torch.FloatTensor),\n",
    "                    offset = self.embedding_solver.model.get_offset(),\n",
    "                    device = device\n",
    "                ),\n",
    "                batch_size = self.batch_size,\n",
    "                shuffle = True\n",
    "            )\n",
    "            self.binaryClassifier = torch.nn.Sequential(\n",
    "                self.model,\n",
    "                torch.nn.Linear(self.latent_dimension,self.latent_dimension),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(self.latent_dimension,1),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(1,1) #Logit output\n",
    "            ).to(device)\n",
    "            self.decoder_solver = SupervisedNNSolver(\n",
    "                model = self.binaryClassifier,\n",
    "                criterion = torch.nn.BCEWithLogitsLoss(),\n",
    "                optimizer = torch.optim.Adam(self.binaryClassifier.parameters(), lr = self.learning_rate)\n",
    "            )\n",
    "            self.decoder_solver.fit(self.simple_train_dataloader, num_steps = self.steps)\n",
    "    def train(self, learning_rate, batch_size, steps, verbose = True):\n",
    "        self.train_embedding(learning_rate, batch_size, steps, verbose)\n",
    "        self.train_decoder(verbose = verbose)\n",
    "    def score(self, verbose = True):\n",
    "        with device:\n",
    "            test_loader = cebra.data.single_session.DiscreteDataLoader(\n",
    "                dataset = self.test_dataset,\n",
    "                num_steps = self.steps,\n",
    "                batch_size = self.batch_size,\n",
    "                prior = \"empirical\"\n",
    "            )\n",
    "            self.embedding_score = self.embedding_solver.validation(test_loader)\n",
    "            self.simple_test_dataloader = torch.utils.data.DataLoader(\n",
    "                SimpleTensorDataset(\n",
    "                    data = self.X_test_tensor.type(torch.FloatTensor),\n",
    "                    labels = self.y_test_tensor.type(torch.FloatTensor),\n",
    "                    offset = self.embedding_solver.model.get_offset(),\n",
    "                    device = device\n",
    "                    ),\n",
    "                batch_size = self.batch_size,\n",
    "                shuffle = True\n",
    "                )\n",
    "            self.decoder_score = self.decoder_solver.validation(self.simple_test_dataloader)\n",
    "            try:\n",
    "                # Fit an exponential decay to the history and get the R2 value\n",
    "                def exp_decay(x, a, b, c):\n",
    "                    return a * np.exp(-b * x) + c\n",
    "\n",
    "                # Fit the exponential decay to the solver history\n",
    "                x_data = np.arange(len(self.embedding_solver.history))\n",
    "                y_data = self.embedding_solver.history\n",
    "                y0 = y_data[0]\n",
    "                yf = y_data[-1]\n",
    "                popt, _ = curve_fit(exp_decay, x_data, y_data, p0=(y0 - yf, 1e-6, yf))\n",
    "\n",
    "                # Calculate the R2 value\n",
    "                y_pred = exp_decay(x_data, *popt)\n",
    "                r2 = r2_score(y_data, y_pred)\n",
    "                if verbose:\n",
    "                    print(f\"Expontial decay R2: {r2}, embedding score: {self.embedding_score}, decoder score: {self.decoder_score}\")\n",
    "                return -r2 + self.embedding_score + 2*self.decoder_score\n",
    "            except:\n",
    "                return float(\"inf\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['offset10-model',\n",
       " 'offset5-model',\n",
       " 'offset1-model',\n",
       " 'offset1-model-v2',\n",
       " 'offset1-model-v3',\n",
       " 'offset1-model-v4',\n",
       " 'offset1-model-v5',\n",
       " 'offset36-model',\n",
       " 'offset36-model-dropout',\n",
       " 'offset36-model-more-dropout']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_to_try = list(filter(lambda x: \"offset\" == x[:6] and not \"mse\" == x[-3:] and not \"subsample\" == x[-9:],cebra.models.get_options()))\n",
    "models_to_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(trial):\n",
    "    model_name = trial.suggest_categorical(\"model_name\", models_to_try)\n",
    "    num_units = trial.suggest_int(\"num_units\", 1, X_train.shape[1])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 50, 512, log=True)\n",
    "    model = ModelPipeline(\n",
    "        model_name,\n",
    "        X_train_tensor=X_train_tensor,\n",
    "        y_train_tensor=y_train_tensor,\n",
    "        X_test_tensor=X_test_tensor,\n",
    "        y_test_tensor=y_test_tensor,\n",
    "        num_units=num_units,\n",
    "        latent_dimension=8\n",
    "    )\n",
    "    model.train(learning_rate, batch_size, 500)\n",
    "    score = model.score()\n",
    "    \n",
    "    # Save the best model to disk\n",
    "    if not hasattr(experiment, \"best_score\") or score < experiment.best_score:\n",
    "        experiment.best_score = score\n",
    "        experiment.pipeline = model\n",
    "        torch.save(model.model.state_dict(), \"./data/models/best_model.pth\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-20 00:10:46,177] A new study created in RDB with name: cebra_offsets\n",
      "pos: -1.1369 neg:  6.7022 total:  5.5653 temperature:  0.8271: 100%|██████████| 500/500 [00:26<00:00, 18.61it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 74.03it/s]\n",
      "/home/icaro/Documents/doctorate/dimensionality-reduction/utils.py:78: UserWarning: You should pass at least one of the arguments 'continuous' or 'discrete'.\n",
      "  warnings.warn(\n",
      "[I 2024-10-20 00:11:21,804] Trial 0 finished with value: 5.271162625314428 and parameters: {'model_name': 'offset1-model-v3', 'num_units': 617, 'learning_rate': 0.0004034283040169691, 'batch_size': 279, 'n_neighbors': 14}. Best is trial 0 with value: 5.271162625314428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expontial decay R2: 0.47565444696226233, embedding score: 5.649832680702209, decoder score: 0.04849219578724051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos: -0.9632 neg:  7.1052 total:  6.1420 temperature:  1.0379: 100%|██████████| 500/500 [16:46<00:00,  2.01s/it]\n",
      "/home/icaro/Documents/doctorate/dimensionality-reduction/utils.py:78: UserWarning: You should pass at least one of the arguments 'continuous' or 'discrete'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(storage=\"sqlite:///data/ID18150/Day2/optuna.db\", study_name=\"cebra_offsets\", direction=\"minimize\", load_if_exists=True)\n",
    "study.optimize(experiment, n_trials=100)\n",
    "\n",
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
